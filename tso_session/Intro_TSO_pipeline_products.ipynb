{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![STSCI banner](https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TSO data products & the JWST TSO pipeline\n",
    "\n",
    "Author: Sarah Kendrew, Instrument & Calibration Scientist, ESA/STScI MIRI Branch <br>\n",
    "Last Updated: 28 Nov 2021<br>\n",
    "Pipeline version: 1.3.3<br>\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Introduction](#intro)<br>\n",
    "   1.1 [Purpose of this Notebook](#purpose)<br>\n",
    "   1.2 [Input Simulations](#inputs)<br>\n",
    "   1.3 [Caveats for Simulated Data](#mirisim)<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.<font color='white'>-</font>Introduction <a class=\"anchor\" id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1<font color='white'>-</font>Purpose of this notebook <a class=\"anchor\" id=\"purpose\"></a>\n",
    "\n",
    "In this notebook we provide a realistic example for running the JWST pipeline on a JWST Time Series Observation (TSO). For the purposes of this tutorial we will work with a simulated observation with the MIRI Low Resolution Spectrometer (LRS). In particular, we focus on aspects of the pipeline that differ from \"standard\" algorihtms and procedures. There will not be enough time to look at every step in detail, but we will demonstrate how to make changes to the pipeline setting (and why you might want to do that) for the best scientific utility. \n",
    "\n",
    "Note that the notebook uses JWST Calibration Pipeline version 1.3.3, which is the current version at the time of this JWebbinar. The pipeline will however be further developed and updated post-launch. \n",
    "\n",
    "We will start with a simple simulated MIRI LRS observation, created using MIRISim version 2.4.1, which is compatible with pipeline version 1.3.3 (https://wiki.miricle.org/Public/MIRISim_Public). The data are described in more detail below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2<font color='white'>-</font>Input Simulations <a class=\"anchor\" id=\"inputs\"></a>\n",
    "\n",
    "We used the MIRISim software package (v2.4.1) to generate realistic simulations of a MIRI LRS slitless observation of a simple stellar-type point source. The stellar SED was modelled as a simple black body spectrum with the following parameters:\n",
    "\n",
    "* Temperature = 6230 K\n",
    "* Normalised to K = 8.99, or flux of 20 mJy at 2 $\\mu$m. \n",
    "\n",
    "LRS slitless observations are carried out in the SLITLESSPRISM subarray of the MIRI Imaging detector. The subarray has 416 rows x 72 columns (the left-most 4 columns are reference pixels, i.e. no illuminated), with sampling of 0.11 arcsec/pix. The single-frame read time for this subarray is 0.159 seconds, and the FASTR1 read mode has an extra reset between integrations. We perform an observation of 100 groups, 10 integrations, in a single exposure; giving an exposure time of:\n",
    "\n",
    "t$_{exp}$ = ((100 + 1) $\\times$ 0.159) $\\times$ 10 = 160.59 s = 2.67 minutes\n",
    "\n",
    "\"Real\" TSOs will typically have many more integrations than 10, with exposures covering many hours. But for the sake of reducing processing time, we use a shorter exposure here. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3<font color='white'>-</font>Caveats for Simulated Data<a class=\"anchor\" id=\"mirisim\"></a> ###\n",
    "\n",
    "As noted above, in this notebook we will be processing simulated data created with the MIRIsim tool.  Like the pipeline, MIRISim is also an evolving piece of software and there are multiple known issues that can cause problems.  \n",
    "\n",
    "**General MIRISim caveats**\n",
    "\n",
    "- Detector noise properties are not modelled in a fully physically realistic way. It's not recommended to use these simulations \"out of the box\" for detailed noise investigations. \n",
    "\n",
    "- Reference pixels are not treated consistently, the refpix step of detector1 must therefore be turned off to process mirisim data without artifacts.\n",
    "\n",
    "- The default detector read mode is FASTR1, which contains a reset between integrations. MIRISim still uses the FAST read mode, which does not include this extra reset. The exposure time recordedin the simulated FITS header is therefore missing this additional reset time. \n",
    "\n",
    "**MIRI TSO-specific caveats**\n",
    "\n",
    "- There is no \"TSO\" flag in MIRISim; the software does not set the header keyword that the pipeline looks for to recignise whether an exposure is a TSO. We will set this manually in this notebook.\n",
    "\n",
    "- MIRISim is not able to insert a time-variable signal into a simulated observation. The source flux is assumed to be constant. Additional tools have been developed in the MIRI consortium for this purpose, but for the aims of this tutorial we do not consider this. \n",
    "\n",
    "- The MIRI LRS prism has a leak in its transmission that causes some 3-4 $\\mu$m flux to contaminate the spectrum around 6-7 $\\mu$m. This is not included in the MIRISim models. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.<font color='white'>-</font>Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we set things up a number of necessary things in order for the pipeline to run successfully:\n",
    "\n",
    "1. import the necessary python packages\n",
    "2. specify the directory structure\n",
    "\n",
    "</div>\n",
    "\n",
    "First the imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set these enviromental variables for this notebook to work properly:\n",
    "%set_env CRDS_PATH $HOME/crds_cache\n",
    "%set_env CRDS_SERVER_URL https://jwst-crds.stsci.edu\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "import astropy.io.fits as fits\n",
    "\n",
    "from jwst import datamodels\n",
    "from jwst.pipeline import calwebb_detector1, calwebb_spec2, calwebb_tso3\n",
    "from jwst.associations.asn_from_list import asn_from_list\n",
    "from gwcs.wcstools import grid_from_bounding_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create an output directory for the new products we produce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = 'miri_lrs_output/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.<font color='white'>-</font>Retrieving and Inspecting the Uncalibrated Data <a class=\"anchor\" id=\"firstlook\"></a>\n",
    "\n",
    "In practice, you will retrieve your observational data from the [MAST archive portal](https://mast.stsci.edu/portal/Mashup/Clients/Mast/Portal.html). Here you can query on a host a different parameters - mission, instrument, program ID, date of execution, etc. \n",
    "\n",
    "In the archive you will find both uncalibrated and calibrated data. All data received from the observatory is automatically processed by the current pipeline version; this includes ancillary data such as target acquisition images. The steps that are run in the pipeline for a particular data mode can be found in the pipeline documentation for [Stage 1](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html#calwebb-detector1), [Stage 2](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html) and [Stage 3](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_tso3.html). \n",
    "\n",
    "This automated pipeline processing uses a set of default parameters that were determined by the instrument teams to be \"good\" for a typical observation, however, some observers may want to try out different settings or find better settings for their particular science case. We will show later how this works. \n",
    "\n",
    "For the purposes of this JWebbinar we provide a simulated observation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'miri_lrs_tso_100G10I_mirisim241.fits'\n",
    "#f = '/home/shared/preloaded-fits/miri_lrs_tso_100G10I_mirisim241.fits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access this file and examine structure & contents using regular astropy FITS tools, like this. You can see the file has 5 extensions:\n",
    "\n",
    "* An empty primary extension (with a header)\n",
    "* SCI: the science data. This is a 4D dataset: NCOLS x NROWS x NGROUPS x NINTS. \n",
    "* PIXELDQ: data quality flags. This is a single 2D plane with dimensions NCOLS x NROWS.\n",
    "* REFOUT: the MIRI reference output values. This is a MIRI specific output. \n",
    "* ASDF: the metadata for the datamodels. \n",
    "\n",
    "Note that normally the PXELDQ plane is added in the first pipeline stage; its presence here is a feature of MIRISIm. Only the reference columns are flagged, the rest of this array is zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu = fits.open(f)\n",
    "hdu.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib, we can visualize the data. The first plot below shows groups 25, 50 and 100 of the first integration of the exposure. You can see how the signal builds up with increasing group number. This increase in signal forms the ''ramp'' that the pipeline will be fitting. \n",
    "\n",
    "The second plot shows this flux build up in 2 pixels: one in the source spectrum, and one in the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_fits = hdu[1].data\n",
    "print(np.shape(uncal_fits))\n",
    "\n",
    "nints = np.shape(uncal_fits)[0]\n",
    "\n",
    "# identifying pixels in source and background regions\n",
    "src_px = [37, 360]\n",
    "bgr_px = [20, 200]\n",
    "px_labels = [\"source + bgr px\", \"bgr px\"]\n",
    "\n",
    "\n",
    "ql_fig, ax = plt.subplots(ncols=3, nrows=1, figsize=[15,8])\n",
    "plt_ints = [24, 49, 99]\n",
    "\n",
    "\n",
    "\n",
    "for i, pi in enumerate(plt_ints):\n",
    "    ax[i].imshow(uncal_fits[8, pi, :, :], origin='lower', aspect='equal', interpolation='None')\n",
    "    ax[i].set_title('Grp {}'.format(pi+1))\n",
    "    sc1 = ax[i].scatter(src_px[0], src_px[1], marker='x', color='cyan')\n",
    "    sc2 = ax[i].scatter(bgr_px[0], bgr_px[1], marker='x', color='magenta')\n",
    "\n",
    "ql_fig.legend([sc1, sc2], labels=px_labels, loc='lower center', fontsize='x-large', ncol=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ramp_fig, ax = plt.subplots()\n",
    "for i in range(nints):\n",
    "    ax.plot(uncal_fits[i,:,src_px[1], src_px[0]], 'c-', lw=2)\n",
    "    ax.plot(uncal_fits[i,:,bgr_px[1], bgr_px[0]], 'm-', lw=2)\n",
    "\n",
    "leg = [Line2D([0], [0], lw=2, color='c', label='source + bgr px'), \n",
    "       Line2D([0], [0], lw=2, color='m', label='bgr px')]   \n",
    "    \n",
    "ax.set_xlabel('group no.', fontsize='large')\n",
    "ax.set_ylabel('DN', fontsize='large')\n",
    "ax.legend(handles=leg, loc=2, fontsize='large')\n",
    "\n",
    "ramp_fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Introduction to JWST datamodels\n",
    "\n",
    "The JWST Calibration Pipeline (\"the pipeline\") provides datamodels for convenient accessing and working with the data. These datamodels are effectively containers that are optimised for particular JWST data types. More information is available [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/datamodels/index.html).\n",
    "\n",
    "You don't need to know what specific model your data corresponds to. The ``datamodels.open`` function checks the relevant header keywords and matches the data against an existing model. \n",
    "\n",
    "In the cells below we will explore come useful aspects of the datamodels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal = datamodels.open(f)\n",
    "print(uncal)\n",
    "print(uncal.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema gives us a first look at the the model attributes, and how the metadata is packaged in the model. Most importantly, the science data is in the ``uncal.data`` attribute.  If you aren't sure where to find a particular keyword, the function ``uncal.find_fits_keyword()`` can identify it for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of groups per integration in the exposure is {0}'.format(uncal.ngroups))\n",
    "print('The number of integrations in the exposure is {0}'.format(uncal.nints))\n",
    "print('Information on the filter used in this exposure can be found here: {0}'.format(uncal.find_fits_keyword('FILTER')))\n",
    "print('OK! So the filter used is {0}, which is the LRS double prism')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recreate the above plot using datamodel syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_fig2, ax2 = plt.subplots(ncols=3, nrows=1, figsize=[15,8])\n",
    "plt_ints = [24, 49, 99]\n",
    "\n",
    "for i, pi in enumerate(plt_ints):\n",
    "    ax2[i].imshow(uncal.data[8, pi, :, :], origin='lower', aspect='equal', interpolation='None')\n",
    "    ax2[i].set_title('Grp {}'.format(pi+1))\n",
    "    sc1 = ax2[i].scatter(src_px[0], src_px[1], marker='x', color='cyan')\n",
    "    sc2 = ax2[i].scatter(bgr_px[0], bgr_px[1], marker='x', color='magenta')\n",
    "\n",
    "ql_fig2.legend([sc1, sc2], labels=px_labels, loc='lower center', fontsize='x-large', ncol=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``meta`` attribute of the model is particular rich in unseful information. We'll show some of its useful features in the next cell. Not all attributes of the model are correctly populated as the MIRISim simulation data lacks some of the JWST observatory keywords. \n",
    "\n",
    "\n",
    "We show here that the \"TSOVISIT\" keyword or attribute is not set. This is a MIRISim issue, and we'll show below how we set this manually in the data model to ensure that the pipeline recognizes the exposure as a TSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total exposure time is {:.2f} seconds'.format(uncal.meta.exposure.exposure_time))\n",
    "print('Detector readout pattern was {0} mode'.format(uncal.meta.exposure.readpatt))      \n",
    "print('The original filename of this RampModel was {0}'.format(uncal.meta.filename))     \n",
    "print('The most amazing space telescope bar none is {0}'.format(uncal.meta.telescope))   \n",
    "print('TSOs should have this attribute set to True and instead it reads {0}'.format(uncal.meta.visit.tsovisit))\n",
    "\n",
    "# Let's change the TSO status\n",
    "if not uncal.meta.visit.tsovisit:\n",
    "    uncal.meta.visit.tsovisit = True\n",
    "\n",
    "print('Now the TSO setting reads {0}'.format(uncal.meta.visit.tsovisit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section, we:<br>\n",
    "* loaded the uncalibrated MIRISim data for a slitless LRS exposure<br>\n",
    "* showed how to load the data using datamodels<br>\n",
    "* showed some of the useful features of datamodels and how metadata is organised in the model attributes<br>\n",
    "</div>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.<font color='white'>-</font>Intro to Running the Pipeline: CalDetector1() <a class=\"anchor\" id=\"pipeline\"></a>\n",
    "\n",
    "In this section we will demonstrate how to run the JWST calibration pipeline on TSO data. When you retrieve your data from the archive, you will have pipeline processed data available for download already; however as part of your research you may well want to rerun the pipeline, or specific steps or stages, to optimise certain settings for your targets.\n",
    "\n",
    "We will highlight here a few pipeline steps that you may want to skip or modify for TSOs, and show how to do that. \n",
    "\n",
    "The pipeline consists of [**3 stages**](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/main.html#pipelines). For TSOs, these are:\n",
    "\n",
    "* CalDetector1: implements basic detector calibrations and converting ramps to slopes\n",
    "* CalImage2 or CalSpec2: performs additional instrument- or mode-level calibrations, returning flux calibrated images for imaging data, and flux calibrated images and extracted spectra for spectroscopic data. \n",
    "* CalTso3: higher level calibrations, returning more TSO-specific output products\n",
    "\n",
    "In the next sections we will run our MIRI LRS data through these pipeline stages, highlighting basic functionality, output products and suggested modifications. We will start with an introduction to the basic call sequence for the pipeline. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Running the Pipeline: Basics\n",
    "\n",
    "Full documentation for running the pipeline from within Python can be found on the [main pipeline documentation pages](https://jwst-pipeline.readthedocs.io/en/latest/jwst/introduction.html#running-from-within-python). In additional, materials for other JWebbinars providing more general instroductions to the pipeline can be found [here](https://www.stsci.edu/jwst/science-execution/jwebbinars).\n",
    "\n",
    "In our import statements above we imported the modules ``calwebb_detector1``, ``calwebb_spec2`` and ``calwebb_tso3`` from ``jwst.pipeline``. The most basic call sequence looks as follows:\n",
    "\n",
    "``det1 = calwebb_detector1.Detector1Pipeline.call(f)`` passing the FITS file to the call sequence; or<br>\n",
    "\n",
    "``det1 = calwebb_detector1.Detector1Pipeline.call(uncal)`` using the previously loaded datamodel as input. <br>\n",
    "\n",
    "In this case we will work with the ``uncal`` datamodel, as we made a modification to it in the above section. Let's see what happens. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(uncal.meta.visit.tsovisit)\n",
    "det1 = calwebb_detector1.Detector1Pipeline.call(uncal, output_dir=outdir, save_results=True)\n",
    "print(det1)\n",
    "\n",
    "# let's look at the output\n",
    "print(os.listdir(outdir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see no errors, the pipeline successfully ran on our data. The print statement for ``det1`` shows that the pipeline has produced an ImagModel with dimensions (416, 72), which are the correct dimensions for our subarray, corresponding to the output file miri_lrs_tso_100G10I_mirisim241_rate.fits. This is the slope image in DN/s, co-added over all integrations. \n",
    "\n",
    "We can see that the output directory contains an additional file, miri_lrs_tso_100G10I_mirisim241_rateints.fits. This is the file that contains the slope images for each integration, which is the product we want to use for TSOs. Let's take a look at the contents of this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rifile = glob.glob(outdir+'/*rateints.fits')\n",
    "print(rifile)\n",
    "\n",
    "rate_ints = datamodels.open(rifile[0])\n",
    "print(rate_ints)\n",
    "\n",
    "sl_fig, axs = plt.subplots(ncols=5, nrows=2, figsize=[15,10])\n",
    "\n",
    "for i,aa in enumerate(axs.flat):\n",
    "    im = aa.imshow(rate_ints.data[i,:,:], origin='lower', aspect='equal', interpolation='None')\n",
    "    aa.set_title('int = {0}'.format(i+1))\n",
    "\n",
    "    \n",
    "sl_fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = sl_fig.add_axes([0.95, 0.2, 0.02, 0.6])\n",
    "cbar = sl_fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.set_label('DN/s', fontsize='x-large')\n",
    "sl_fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's now dig a bit deeper in what the pipeline is doing to learn more about reference files and to prepare for making some changes. The steps the pipeline runs for a particular mode are defined in a [parameter file in ASDF format](https://jwst-pipeline.readthedocs.io/en/latest/jwst/stpipe/config_asdf.html#config-asdf-files) (ASDF stands for Advanced Scientific Data Format). Asdf file can be read with a simple text editor, and a python package ``asdf`` exists to access programmatically.  \n",
    "\n",
    "There's a handy function called ``get_crds_parameters`` that prints out all the metadata in the model, including information on the provenance of the file, data units, the calibration steps run (the ``cal_step`` entries), the version of the pipeline that was used (``calibration_software_version``), the names of the reference files used for calibration (``ref_file`` entries) and more.\n",
    "\n",
    "Reference files are used to perform the calibration steps. These files are usually delivered by the instrument teams and the contents can be in the form of images or . Examples of reference files used by the pipeline are: \n",
    "* flat fields\n",
    "* flux calibration tables\n",
    "* wavelength calibration files\n",
    "\n",
    "and many more.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_ints.get_crds_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Retrieving and examining reference files\n",
    "\n",
    "Reference files and parameter files for the calibration pipeline live in the **JWST Calibration Reference Data System** - [CRDS](https://jwst-pipeline.readthedocs.io/en/latest/jwst/introduction.html#crds). You can see this acronym referenced throughout the pipeline documentation, and in the metadata for the exposures. \n",
    "\n",
    "Each version of the pipeline has an associated CRDS **context**; you can see the context used in our Detector1Pipeline run in ``rate_ints.meta.ref_file.crds.context_used`` as ``jwst_0776.pmap``. Sometimes it may be necessary to change this setting, in order to run the pipeline with older reference files. \n",
    "\n",
    "The CRDS system can be accessed programmatically, or simply via the following URL: https://jwst-crds.stsci.edu, from where you can download reference files with a simple click. In the figures below you can see the web interface for CRDS, and how we can locate the parameter ASDF files for individual pipeline stages and steps. See if you can retrieve the ASDF file used for our Detector1Pipeline run?\n",
    "\n",
    "To save on time, we have included a new read noise reference file in our working directory (which is actually a copy of the existing one), and will demonstrate how to override the reference file in the pipeline call. \n",
    "\n",
    "![Fig 1 CRDS interface](images/crds_interface.png)\n",
    "\n",
    "![Fig 2 Det1 parameter file](images/crds_miri_det1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we've now seen how to run the pipeline with its default settings, and where to find the information about those settings. Via the datamodel metadata we can see the filenames of any reference files that were used, the CRDS context. We can access these files from a web interface or programmatically. \n",
    "\n",
    "Next, let's see how we can make changes to these settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Making changes to the pipeline steps\n",
    "\n",
    "It will be a pretty common occurrence that you want to investigate the impact of a particular calibration step on your analysis, change a parameter, or try using a different reference file. In this section we'll look at how to do that. \n",
    "\n",
    "__First__, we will take the read noise reference file in the working directory, and rerun the ``Detector1Pipeline()`` step with the new file. This demonstrates how a reference file can be _overridden_. The read noise reference file is references in both the jump step and in the ramp fitting. \n",
    "\n",
    "(**Note:** The basic detector calibration reference files were created following very detailed ground testing and often lengthy detector investigations. We show here how to override the read noise file to demonstrate the _method_; we recommend you replace Detector1 reference files with _extreme caution_). \n",
    "\n",
    "__Second__, we would like the ``Detector1Pipeline()`` to run the ``lastframe`` step. The MIRI detectors have an odd pull-down effect on the last frame (group) in an integration. This introduces a deviation from the (quasi-) linear response of the detector, and as the effect has an odd-even row dependence, it can impact line measurements, especially if we are looking at line ratios. However, this effect is also stable over time for a given source, so for TSOs we prefer to use the last frame for maximal SNR. But if we consider a situation where the absolute flux calibration _is_ important - for example to match the data with that from another instrument, then we may want to rerun the pipeline to include the step (and _exclude_ the last frame in the ramp fit). \n",
    "\n",
    "\n",
    "We're going to set the ``save_results`` parameter to ``False``, as we're not going to use these output files any further and we don't need to save them. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 1: identify the new read noise file\n",
    "new_readnoise = 'miri_new_readnoise_file.fits'\n",
    "\n",
    "\n",
    "# Bring it all together\n",
    "d1mod = calwebb_detector1.Detector1Pipeline.call(uncal, save_results=False, \n",
    "                                           steps={'jump': {'override_readnoise': new_readnoise},\n",
    "                                                  'ramp_fit': {'override_readnoise': new_readnoise},\n",
    "                                                  'lastframe': {'skip': False}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check if our changes were applied\n",
    "print('Read noise reference file used was {0}'.format(d1mod.meta.ref_file.readnoise.name))\n",
    "print('The last frame step status: {0}'.format(d1mod.meta.cal_step.lastframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section, we ran the ``calwebb_detector1`` pipeline, and learned a few basic operations:<br>\n",
    "<br>\n",
    "* running a pipeline stage end-to-end with default settings, using the ``Step.call()`` method<br>\n",
    "* identifying and visualizing the output products<br>\n",
    "* showing the list of reference files used in the various calibration steps, and how to retrieve them in CRDS<br>\n",
    "* make basic modifications to the pipeline call, such as skipping steps or overriding a reference file.<br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='white'>-</font>5. Progressing Further: Spec2Pipeline()<a class=\"anchor\" id=\"spec2pipe\"></a>\n",
    "\n",
    "We have demonstrated some basics of the pipeline using Stage 1 of the Pipeline, which has converted ramps to slopes for each integration in the exposure. In this section we will run our data through Stage 2 of the pipeline, showing the following methods:\n",
    "\n",
    "* Running and configuring individual steps\n",
    "* Inspecting the WCS/wavelength information\n",
    "* Spectral extraction and making modifications\n",
    "\n",
    "\n",
    "### 5.1 Introduction to the Stage 2 Pipeline\n",
    "\n",
    "In Stage 2 of the pipeline, the following calibrations are applied to our MIRI LRS data:\n",
    "\n",
    "1. Assigning a world coordinate system (including wavelengths)\n",
    "2. Assigning a source type\n",
    "3. Flat fielding\n",
    "4. Photometric calibration\n",
    "5. Spectral extraction\n",
    "\n",
    "The Stage 2 pipeline has many more steps, but many do not apply to TSOs. For the full list of steps and which instruments and modes use what steps, see the [pipeline doumentation pages](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html#calwebb-spec2). \n",
    "\n",
    "Note that imaging TSOs use the Image2Pipeline(), [described here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_image2.html).\n",
    "\n",
    "\n",
    "### 5.2 Running the pipeline step by step\n",
    "\n",
    "In the previous section we called the Stage 1 pipeline as a single step, via ``calwebb_detector1.Detector1Pipeline.call()``. This method works also for Stage 2, using the ``rateints.fits`` file as input. The call sequence is as follows:\n",
    "\n",
    "``spec2_out = calwebb_spec2.Spec2Pipeline.call(rate_ints, save_results=True, output_dir=out_dir)``\n",
    "\n",
    "However in this section we will demonstrate how to run individual steps one by one, which can make it easier to configure and modify steps.\n",
    "\n",
    "Let's first import the individual steps of the Stage 2 pipeline and some additional ``gwcs`` tools. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.assign_wcs import AssignWcsStep\n",
    "from jwst.srctype import SourceTypeStep\n",
    "from jwst.flatfield import FlatFieldStep\n",
    "from jwst.photom import PhotomStep\n",
    "from jwst.extract_1d import Extract1dStep\n",
    "\n",
    "from gwcs.wcstools import grid_from_bounding_box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Assign_wcs\n",
    "\n",
    "OK, that's all imported. Let's start with the ``assign_wcs`` step. We can accept all default settings for this step. This step makes no modifications to the science data. It only attaches WCS information to the data in a WCS extension, including wavelength, such that every pixel in the detector array has an associated (RA, Dec, wavelength). It also defines a bounding box region over which these values are defined (with NaNs everywhere else). \n",
    "\n",
    "**NOTE**. The assignment of wavelength values to pixels assumes that the target is placed at a specific location, i.e. the nominal pointing location for the SLITLESSPRISM subarray. The target acquisition procedure should be capable of placing the target there with an accuracy of < 10 mas, and an image will be taken through the TA filter which you will have access to, to verify the source positioning. For reference, the pixel scale of the MIRI Imager detector is 110 mas/px, so 10 mas is approx. 1/10th of a pixel. \n",
    "\n",
    "Any offset of the target from this location will result in a small wavelength offset in the final spectrum. The TA performance estimates represent pre-launch values; TA inaccuracies can also result from inaccurate target coordinates, partial saturation of the target in the TA exposure, or insufficient SNR.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awcs = AssignWcsStep.call(rate_ints, save_results=True, output_dir=outdir)\n",
    "print(awcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ``print`` statement above we can see that the output data is a CubeModel, still with the dimensions of the ``rateints.fits`` file. The step has produced a new output file, ``miri_lrs_tso_100G10I_mirisim241_assignwcsstep.fits``, which we can also access via the step output, ``awcs``. \n",
    "\n",
    "Let's look at the bounding box that was defined on the aperture, and how we can retrieve a wavelength map. For this we need to import a function from the ``gwcs`` package that comes with the JWST pipeline. We use this to plot a 2D wavelength map of the subarray. As MIRISim simulated data do not contain realistic coordinates, we do not show the RA, Dec - but the method shown works for this too with on-sky data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Corner pixel coordinates of the bounding box: {0}'.format(awcs.meta.wcs.bounding_box))\n",
    "\n",
    "bbox_w = awcs.meta.wcs.bounding_box[0][1] - awcs.meta.wcs.bounding_box[0][0]\n",
    "bbox_ht = awcs.meta.wcs.bounding_box[1][1] - awcs.meta.wcs.bounding_box[1][0]\n",
    "print('Bounding box height: {0} px; width: {1} px'.format(bbox_ht, bbox_w))\n",
    "\n",
    "# Now let's calculate the grid\n",
    "x,y = grid_from_bounding_box(awcs.meta.wcs.bounding_box)\n",
    "ra, dec, lam = awcs.meta.wcs(x, y)\n",
    "\n",
    "bbfig, bbax = plt.subplots(ncols=2, nrows=1, figsize=[8,9])\n",
    "bbox = Rectangle((awcs.meta.wcs.bounding_box[0][0],awcs.meta.wcs.bounding_box[1][0]), bbox_w, bbox_ht, angle=0.0, ec='r', lw=2, fc='None')\n",
    "\n",
    "bbax[0].imshow(awcs.data[1,:,:], origin='lower', interpolation='None', aspect='equal')\n",
    "bbax[0].add_patch(bbox)\n",
    "bbax[0].set_title('Bounding box placement', fontsize='large')\n",
    "\n",
    "wim = bbax[1].imshow(lam, origin='lower', aspect='equal', interpolation='None')\n",
    "bbax[1].set_title('Wavelength map', fontsize='large')\n",
    "bbfig.colorbar(wim, ax=bbax[1], label='micron')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Source type\n",
    "\n",
    "The pipeline assigns a source type based on information passed from the APT proposal. If no information is available from APT, then each mode has a default value. For MIRI slitless LRS this is 'POINT'. This step is pretty straightforward, we will run it without any further modifications. Remember to use the output of the previous step as input; this should be the FITS filename. We can get the filename out the output product of the ``assign_wcs`` step from the datamodel metadata using ``awcs.meta.filename``.\n",
    "\n",
    "Whether the target is POINT or EXTENDED becomes important later during the flux calibration and spectral extraction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(awcs.meta.filename)\n",
    "srctype_input = outdir+awcs.meta.filename\n",
    "print('Original value of the source type metdata: {0}'.format(awcs.meta.target.source_type))\n",
    "\n",
    "spec2_src = SourceTypeStep.call(srctype_input, save_results=True, output_dir=outdir)\n",
    "\n",
    "print('Value after the source type step: {0}'.format(spec2_src.meta.target.source_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Flat fielding\n",
    "\n",
    "In the next step we apply the flat field correction; this corrects for the gain differences between pixels. The gain uniformity of the MIRI detectors is overall very good. We will run this step without further modifications or comments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_input = outdir+spec2_src.meta.filename\n",
    "spec2_ff = FlatFieldStep.call(ff_input, save_results=True, output_dir=outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Photometric calibration\n",
    "\n",
    "In the photometric calibration step (\"photom\"), we perform the important calibration of converting the units of the science (and error) data from DN/S to MJy/sr. \n",
    "\n",
    "**NOTE**. It is important to note that the reference file used for this step provides conversion factors as a function of _wavelength_. As noted above, the assignment of wavelengths to pixels, as performed in the ``assign_wcs`` step assumes a specific location of the target on the detector. This means that if the target is offset from this location, the flux calibration will be affected. \n",
    "\n",
    "The pointing stability of the telescope over timescales of many hours, typical of some TSO exposures, is not fully characterzed at this pre-launch stage. Any jitter or slow drifting behaviour will result in small errors in the flux calibration of the target, which may introduce noise in the further time series analysis.  \n",
    "\n",
    "As a result, the value of this step is very much dependent on your science goals and the type of target. When performing relative spectrophotometry only, e.g. detecting an exoplanet transit against an out-of-transit baseline, it may be prudent to skip this step (or at least carefully consider the impact of this issue).\n",
    "\n",
    "If you decide to skip the step, you can jump from flat fielding to spectral extraction without any issues; the spectra will be extracted in units of DN/s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data units prior to the Photom step: {0}'.format(spec2_ff.unit))\n",
    "photom_input = outdir+spec2_ff.meta.filename\n",
    "spec2_ph = PhotomStep.call(photom_input, save_results=True, output_dir=outdir)\n",
    "print('Data units after the Photom step: {0}'.format(spec2_ph.unit))\n",
    "print('Output filename: {0}'.format(spec2_ph.meta.filename))\n",
    "np.shape(spec2_ph.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5. Extract 1D\n",
    "\n",
    "In the final step of the Stage 2 pipeline, the 2D spectral images are extracted into 1D, flux-calibrated spectra. For TSOs, the output product will contain an extracted spectrum for each integration, so that a spectroscopic time series can be constructed. \n",
    "\n",
    "For MIRI LRS, the default extraction method is a fixed-width aperture measuring 11 pixels across. Wavelength dependent aperture correction factors are applied as part of the algorithm. The error arrays are also extracted and the errors combined to give an uncertainty on the science spectrum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1d_input = outdir+spec2_ph.meta.filename\n",
    "x1d = Extract1dStep.call(x1d_input, save_results=True, output_dir=outdir)\n",
    "print(x1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datamodel of the ``extract_1d`` output is a MultiSpecModel where the main attribute ``spec`` is a list of SpecModels - one for each integration.\n",
    "\n",
    "When we plot the spectra below, we can see that the spectrum increases at long wavelengths - not exactly what you'd expect for a star. This is the thermal background, which starts to dominate over the target flux beyond 10 micron. So far, the background has not been subtracted from our data. Below we show how to do this as part of the spectral extraction step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x1d.spec))\n",
    "\n",
    "x1dfig, x1dax = plt.subplots(figsize=[12,4])\n",
    "\n",
    "for i in range(len(x1d.spec)):\n",
    "    x1dax.plot(x1d.spec[i].spec_table['WAVELENGTH'], x1d.spec[i].spec_table['FLUX'])\n",
    "    \n",
    "x1dax.set_title('Extracted spectra (per integration)')\n",
    "x1dax.set_xlabel('wavelength ($\\mu$m)')\n",
    "x1dax.set_ylabel('flux (Jy)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the ``get_crds_parameters()`` function on the output of this step then we can see that much more metadata was added to the model. It includes also the reference file used for the extraction, `jwst_miri_extract1d_0004.json`, which we can retrieve from CRDS. We show below how to do this programmatically using the ``crds`` package. \n",
    "\n",
    "You can find the full documentation for this package [online here](https://jwst-crds.stsci.edu/static/users_guide/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1d.get_crds_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crds\n",
    "import json\n",
    "\n",
    "x1d_ref_file = (x1d.meta.ref_file.extract1d.name).split('/')[-1]\n",
    "print(x1d_ref_file)\n",
    "\n",
    "basename = crds.core.config.pop_crds_uri(x1d_ref_file)\n",
    "print(basename)\n",
    "\n",
    "file_path = crds.locate_file(basename, \"jwst\")\n",
    "x1d_ref = json.load(open(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1d_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the contents of this file, we see it contains some settings for both the fixed-slit and slitless modes of the MIRI LRS. The parameters in this file set a very basic extraction scheme, with a fixed extraction aperture of 11 pixels. The Extract1D code in the pipeline does support many more extraction options; these are all described in the [pipeline documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html).\n",
    "\n",
    "In the next section, we will show how to implement a simple background subtraction in the spectral extraction step, by defining a background region in the input json file. The simplest way to edit the settings in this file are to copy the file and edit it in a text editor, following the syntax of the existing text.\n",
    "\n",
    "We will perform a background subtraction by defining a simple rectangular background region  of the same width as the source extraction aperture, and ask the code to median the values as a function of wavelength. More complex schemes are possible, such as regions defined by a polynomial. \n",
    "\n",
    "A new json file with these additional background settings is included in the working directory of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1d2 = Extract1dStep.call(x1d_input, override_extract1d='jwst_miri_extract1d_slitless_withbgr.json', save_results=True, output_dir=outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1dfig2, x1dax2 = plt.subplots(figsize=[12,4])\n",
    "\n",
    "for i in range(len(x1d2.spec)):\n",
    "    x1dax2.plot(x1d2.spec[i].spec_table['WAVELENGTH'], x1d2.spec[i].spec_table['FLUX'])\n",
    "    \n",
    "x1dax2.set_title('Extracted spectra (per integration): Background subtracted', fontsize=14)\n",
    "x1dax2.set_xlabel('wavelength ($\\mu$m)', fontsize='large')\n",
    "x1dax2.set_ylabel('flux (Jy)', fontsize='large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3 Comparison running the pipeline end to end\n",
    "\n",
    "We can also run the Spec2Pipeline() end to end. We illustrate this here and show that the output products are the same.\n",
    "\n",
    "Note that the default output file extensions are different when you run the pipeline step by step, versus end to end in one call. \n",
    "\n",
    "* the output of the ``photom`` step will be ``_photomstep.fits`` when run individually, vs. ``_calints.fits`` when run end to end.\n",
    "* similarly the output of ``extract1d`` is ``_extract1dstep.fits``, vs. ``_x1d.fits`` when run end to end.\n",
    "\n",
    "\n",
    "After the run, we plot the extracted spectra from the 1st integration from both methods together, to show that the result is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec2_e2e = calwebb_spec2.Spec2Pipeline.call(rate_ints, save_results=True, output_dir=outdir)\n",
    "print(spec2_e2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = outdir+'miri_lrs_tso_100G10I_mirisim241_x1dints.fits'\n",
    "\n",
    "x1d2 = datamodels.open(xf)\n",
    "\n",
    "\n",
    "x1dfig2, x1dax2 = plt.subplots(figsize=[12,4])\n",
    "\n",
    "\n",
    "x1dax2.plot(x1d.spec[0].spec_table['WAVELENGTH'], x1d.spec[0].spec_table['FLUX'], 'b-', label='from extract1dstep.fits')\n",
    "x1dax2.plot(x1d2.spec[0].spec_table['WAVELENGTH'], x1d2.spec[0].spec_table['FLUX'], 'm-', label='from x1dints.fits')\n",
    "    \n",
    "x1dax2.set_title('Extracted spectra (per integration)')\n",
    "x1dax2.set_xlabel('wavelength ($\\mu$m)')\n",
    "x1dax2.set_ylabel('flux (Jy)')\n",
    "\n",
    "x1dfig2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "In this section, we've run the Stage 2 pipeline on MIRI slitless LRS data, showing the different pipeline steps. In addition we have shown:<br>\n",
    "<br>   \n",
    "\n",
    "* how to run the pipeline step by step and make changes to the step parameters<br>\n",
    "* how to construct and inspect a 2D wavelength map from the WCS information<br>\n",
    "* how spectra are extracted based on a json parameters file, and how this file can be overridden with custom settings<br>\n",
    "* how to perform a sinple background subtraction as part of the extract1d step. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='white'>-</font>6. Final Steps: Tso3Pipeline()<a class=\"anchor\" id=\"tso3pipe\"></a>\n",
    "\n",
    "The 3rd and final stage of the pipeline performs some extra cleanup, and produces higher-level science products. For \"regular\" observations, this is where dithererd exposures are combined, mosaics are produced, etc. For TSOs, you can read about the extra steps performed in the [pipeline documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_tso3.html#calwebb-tso3).\n",
    "\n",
    "* Outlier detection: this step performs some extra cleaning on the images\n",
    "* Spectral extraction: the spectral extraction is repeated on the cleaned images\n",
    "* White light curve creation: produces a white light curve\n",
    "\n",
    "The most important part of the Tso3 pipeline is that it re-combines lengthy exposures that were segmented by the data management system back into a single product. The spectral extraction will therefore at this stage contain spectra for ALL integrations in the exposure. \n",
    "\n",
    "If your exposure was short enough to be captured in a single FITS file, only the white light step will provide significant added value over the Spec2 outputs. \n",
    "\n",
    "### 6.1 Association files\n",
    "\n",
    "An important aspect of the Tso3 pipeline is that it does not accept a single file or datamodel as input; instead we have to create and pass an [association file](https://jwst-pipeline.readthedocs.io/en/latest/jwst/associations/index.html). This is type of text file with specific formatting requirements that points the pipeline to the files and how they relate to each other. In the case of a segmented exposure, we would pass the list of all ``_calints.fits`` files to the association file; for a short exposure that is not segmented, the association file is essentially just a wrapper around the exposure. \n",
    "\n",
    "Fortunately there are tools available to help you create this file and pass it to the pipeline. We demonstrate how to do this below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.associations import asn_from_list\n",
    "\n",
    "#First we create a list of calints.fits files\n",
    "calints_files = glob.glob(outdir+'*calints.fits')\n",
    "print(calints_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn = asn_from_list.asn_from_list(calints_files, rule=DMS_Level3_Base, product_name='miri_lrs_tso_stage3.fits')\n",
    "print(asn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_file = 'miri_lrs_tso_stage3_asn.json'\n",
    "\n",
    "with open(asn_file, 'w') as fp:\n",
    "    fp.write(asn.dump()[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a moment to open this file in a text editor, and look at its contents. \n",
    "\n",
    "\n",
    "### 6.2 Running the Tso3 Pipeline\n",
    "\n",
    "Next we will run the Stage 3 pipeline with this file as input. We make 2 modifications:\n",
    "\n",
    "* we use the extract_1d parameters file created above, to include background subtraction\n",
    "* we set the limits for the white light creation from 6 to 10 $\\mu$m, instead of the default choice of 5-12 $\\mu$m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tso3 = calwebb_tso3.Tso3Pipeline.call(asn_file, save_results=True, output_dir=outdir, \n",
    "                                     steps={'extract_1d': {'override_extract1d' : 'jwst_miri_extract1d_slitless_withbgr.json'},\n",
    "                                             'white_light': {'min_wavelength': 6.0, 'max_wavelength': 10.}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output folder we can see two new files that were created by this pipeline stage, each with the name prefix ``miri_lrs_tso_stage3``, as we specified when we created the association:\n",
    "\n",
    "* miri_lrs_tso_stage3_x1dints.fits\n",
    "* miri_lrs_tso_stage3_whtlt.ecsv\n",
    "\n",
    "We will leave the inspection of the x1dints product as an offline exercise, as this product has the same structure as the output from the Spec2Pipeline. We will take a closer look at the white light curve. The ecsv format is readable by common astropy I/O tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtlt_file = glob.glob(outdir+'*whtlt*')\n",
    "print(wtlt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.io.ascii as ascii\n",
    "\n",
    "wtlt = ascii.read(wtlt_file[0])\n",
    "print(wtlt.colnames)\n",
    "\n",
    "wtfig, wtax = plt.subplots(figsize=[12,4])\n",
    "wtax.plot(wtlt['MJD'], wtlt['whitelight_flux'])\n",
    "wtax.set_xlabel('MJD')\n",
    "wtax.set_ylabel('Flux (Jy)')\n",
    "wtax.set_ylim([0.19, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we ran our simulated observations through Stage 3 of the pipeline, which combines segmented files into 1, re-extracts the data and creates a white light curve by summing over wavelengths. We demonstrated:\n",
    "<br>    \n",
    "* How to produce an association file for the Tso3 pipeline<br>\n",
    "* How to customise the white light limit wavelengths<br>\n",
    "* How to read in and plot a white light curve.<br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.<font color='white'>-</font>Conclusion<a class=\"anchor\" id=\"bye-bye\"></a>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This concludes the 2nd part of our JWebbinar focused on TSOs. We hope this was a useful level-zero introduction to how the pipeline handles TSOs, and how and why you can modify it to get the best quality data. <br>\n",
    "    \n",
    "In the next portion of the JWebbinar, we will take a deeper dive into some of these steps and calibration procedures, highlighting topics that are particularly relevant for TSO science. <br>\n",
    "        \n",
    "As always, if you have any questions or concerns, you can find us through the [JWST Help Desk](https://stsci.service-now.com/jwst). <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
