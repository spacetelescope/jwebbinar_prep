{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c89d8",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a6ac",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI MRS Batch Processing Notebook #\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fb85c",
   "metadata": {},
   "source": [
    "**Author**: David Law, AURA Associate Astronomer, MIRI branch\n",
    "<br>\n",
    "Kirsten Larson, ESA/AURA Astronomer, MIRI branch\n",
    "<br>\n",
    "**Last Updated**: November 22, 2023\n",
    "<br>\n",
    "**Pipeline Version**: 1.12.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f765",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a framework for processing generic MIRI MRS data through all three pipeline stages.  Data is assumed to be located in two Observation folders (science and background) according to paths set up below.  It should not be necessary to edit any cells other than in the 'Configuration' section unless modifying the standard pipeline processing for MIRI MRS.\n",
    "\n",
    "This example is set up to use observations of the LMC planetary nebula SMP LMC 058 obtained by PID 1523 Observation 3.  This is a point source that uses a standard 4-pt dither in all three grating settings.  It incorporates a dedicated background in Observation 4.  Input data for this notebook can be obtained by downloading the 'uncal' files from MAST and placing them in directories set up below.\n",
    "\n",
    "This notebook is regularly updated as improvements are made to the pipeline. Find the most up to date version of this notebook at:\n",
    "https://github.com/STScI-MIRI/MRS-ExampleNB/blob/main/Flight_Notebook1/MRS_FlightNB1.ipynb\n",
    "\n",
    "\n",
    "Changes:<br>\n",
    "Sep 1 2022: Add some commentary and example on how to use multicore processing in Detector 1<br>\n",
    "Sep 12 2022: Disable unnecessary cube/1d spectra production for individual science exposures in Spec 2<br>\n",
    "Oct 14 2022: Include residual fringe correction in spec2 (note that this will CRASH earlier pipeline versions!<br>\n",
    "Jun 29 2023: Update to latest 1.11.0 pipeline with photom, outlier detection, and x1d changes, add CRDS path options.  Change to SMP LMC 058 demo.<br>\n",
    "Oct 11 2023: Update to 1.12.3 pipeline with 1d spectral residual fringe and auto-centroid options, 2d pixel replacement, and a variety of new cube build options.<br>\n",
    "Nov 17 2023: Incorporate CRDS default parameters for detector1 and spec2<br>\n",
    "Nov 20 2023: Significant revisions to allow use of associations in spec2, allow choice of pixel-based background subtraction.<br>\n",
    "Nov 24 2023: Additional comments and plotting examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9868",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410627e4-5d57-422e-bdf1-d53cca12c439",
   "metadata": {},
   "source": [
    "1.<font color='white'>-</font>Configuration <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters to be changed here.\n",
    "# It should not be necessary to edit cells below this in general unless modifying pipeline processing steps.\n",
    "\n",
    "import sys,os, pdb\n",
    "\n",
    "# -----------------------------    Set CRDS context and paths   ----------------------------\n",
    "# CRDS context (if overriding)\n",
    "%env CRDS_CONTEXT  jwst_1146.pmap\n",
    "\n",
    "# Set CRDS paths if not set already in your .bashrc shell configuration\n",
    "os.environ['CRDS_PATH'] = \"crds_cache\"  #use this path for Workshop\n",
    "os.environ['CRDS_SERVER_URL']='https://jwst-crds.stsci.edu'\n",
    "# Echo CRDS path in use\n",
    "print('CRDS local filepath:',os.environ['CRDS_PATH'])\n",
    "print('CRDS file server:',os.environ['CRDS_SERVER_URL'])\n",
    "\n",
    "# -----------------------------    Set Directories   ----------------------------\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the science observation\n",
    "input_dir = 'miri/run_folder/Obs03_sci/uncal/'\n",
    "\n",
    "# Point to where you want the output science results to go\n",
    "output_dir = 'miri/run_folder/Obs03_sci/'\n",
    "\n",
    "# Point to where the uncalibrated FITS files are from the background observation\n",
    "# If no background observation, leave this blank\n",
    "input_bgdir = 'miri/run_folder/Obs04_bkg/uncal/'\n",
    "\n",
    "# Point to where the output background observations should go\n",
    "# If no background observation, leave this blank\n",
    "output_bgdir = 'miri/run_folder/Obs04_bkg/'\n",
    "\n",
    "# -----------------------------    Set Processing Steps   ----------------------------\n",
    "\n",
    "# Whether or not to process only data from a given band/channel\n",
    "# Useful if overriding reference files\n",
    "# Note BOTH must be set in order to work\n",
    "use_ch='12' # '12' or '34'\n",
    "use_band='MEDIUM' # 'SHORT', 'MEDIUM', or 'LONG'\n",
    "\n",
    "# Whether or not to run a given pipeline stage:\n",
    "\n",
    "# Science processing\n",
    "dodet1=True\n",
    "dospec2=True\n",
    "dospec3=True\n",
    "\n",
    "# Background processing\n",
    "dodet1bg=True\n",
    "dospec2bg=True #needed for Master Background subtraction\n",
    "\n",
    "# Where should background subtraction using the dedicated backgrounds be done?\n",
    "# (Note that if using master-background subtraction, backgrounds must be select above to process through spec2)\n",
    "pixel_bg = False # Pixel-based background subtraction in spec2 (direct pixel subtraction) -Deep background exposures needed to not add noise.\n",
    "master_bg = True # Master-background subtraction in spec3 (subtract spectrum generated from the backgrounds) -This is the default pipeline setting\n",
    "\n",
    "# (If none of the above is done, cubes will not be background subtracted.  1d spectra will use local annular background subtraction for point sources)\n",
    "\n",
    "# If there is no background folder, ensure we don't try to process it\n",
    "if (input_bgdir == ''):\n",
    "    dodet1bg=False\n",
    "    dospec2bg=False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457fc0dc-eb72-4178-a946-4fb1cf6eca9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c3a015b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c995",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Imports and setup <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the entire available screen width for the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "#----------------------------------------------General Imports-----------------------------------------------------\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import zipfile\n",
    "import urllib.request\n",
    "# Numpy for doing calculations\n",
    "import numpy as np\n",
    "\n",
    "#--------------------------------------------Astropy Imports--------------------------------------------\n",
    "# Astropy utilities for opening FITS and ASCII files\n",
    "from astropy.io import fits\n",
    "from astropy.io import ascii\n",
    "from astropy.utils.data import download_file\n",
    "# Astropy utilities for making plots\n",
    "from astropy.visualization import (LinearStretch, LogStretch, ImageNormalize, ZScaleInterval)\n",
    "\n",
    "#------------------------------------------------Plotting Imports--------------------------------------------------\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------JWST Calibration Pipeline Imports-------------------------------------------\n",
    "# Import the base JWST package\n",
    "import jwst\n",
    "import crds\n",
    "\n",
    "# JWST pipelines (encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "#from jwst.extract_1d import Extract1dStep     #Extract1D Individual Step\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from jwst import datamodels # JWST datamodels\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level2_base import DMSLevel2bBase # Definition of a Lvl2 association file\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file\n",
    "\n",
    "from stcal import dqflags # Utilities for working with the data quality (DQ) arrays\n",
    "\n",
    "\n",
    "#--\n",
    "print(\"JWST Calibration Pipeline Version={}\".format(jwst.__version__))\n",
    "print(\"Current Operational CRDS Context = {}\".format(crds.get_default_context()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fe0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subdirectories to keep science data products organized\n",
    "# Note that the pipeline might complain about this as it is intended to work with everything in a single\n",
    "# directory, but it nonetheless works fine for the examples given here.\n",
    "det1_dir = os.path.join(output_dir, 'stage1/') # Detector1 pipeline outputs will go here\n",
    "spec2_dir = os.path.join(output_dir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "spec3_dir = os.path.join(output_dir, 'stage3/') # Spec3 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if not os.path.exists(det1_dir):\n",
    "    os.makedirs(det1_dir)\n",
    "if not os.path.exists(spec2_dir):\n",
    "    os.makedirs(spec2_dir)\n",
    "if not os.path.exists(spec3_dir):\n",
    "    os.makedirs(spec3_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subdirectories to keep background data products organized\n",
    "det1_bgdir = os.path.join(output_bgdir, 'stage1/') # Detector1 pipeline outputs will go here\n",
    "spec2_bgdir = os.path.join(output_bgdir, 'stage2/') # Spec2 pipeline outputs will go here\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "if (output_bgdir != ''):\n",
    "    if not os.path.exists(det1_bgdir):\n",
    "        os.makedirs(det1_bgdir)\n",
    "    if not os.path.exists(spec2_bgdir):\n",
    "        os.makedirs(spec2_bgdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a7f86",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c9165",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>Detector1 Pipeline <a class=\"anchor\" id=\"det1\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the Detector1 pipeline to create Lvl2a data products (i.e., uncalibrated slope images).\n",
    "    \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll define a function that will call the detector1 pipeline with our desired set of parameters\n",
    "# We won't enumerate the individual steps\n",
    "def rundet1(filename, outdir):\n",
    "    print(filename)\n",
    "    # Set default configuration from CRDS param reference files\n",
    "    # -This is required when running the pipeline in a function.\n",
    "    crds_config = Detector1Pipeline.get_config_from_reference(filename)\n",
    "    det1 = Detector1Pipeline.from_config_section(crds_config)\n",
    "    \n",
    "    det1.output_dir = outdir # Specify where the output should go\n",
    "    \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #det1.dq_init.skip = False\n",
    "    #det1.saturation.skip = False\n",
    "    #det1.firstframe.skip = False\n",
    "    #det1.lastframe.skip = False\n",
    "    #det1.reset.skip = False\n",
    "    #det1.linearity.skip = False\n",
    "    #det1.rscd.skip = False\n",
    "    #det1.dark_current.skip = False\n",
    "    #det1.refpix.skip = False\n",
    "    #det1.jump.skip = False\n",
    "    #det1.ramp_fit.skip = False\n",
    "    #det1.gain_scale.skip = False\n",
    "    \n",
    "    # The jump and ramp fitting steps can benefit from multi-core processing, but this is off by default\n",
    "    # Turn them on here if desired by choosing how many cores to use (quarter, half, or all)\n",
    "    det1.jump.maximum_cores='half'\n",
    "    det1.ramp_fit.maximum_cores='half'\n",
    "    # This next parameter helps with very bright objects and/or very short ramps\n",
    "    det1.jump.three_group_rejection_threshold=100\n",
    "    \n",
    "    # Enable detection of large cosmic ray showers (currently only works for FASTR1 data)\n",
    "    det1.jump.find_showers=True\n",
    "    \n",
    "    # Bad pixel mask overrides\n",
    "    #det1.dq_init.override_mask = 'myfile.fits'\n",
    "\n",
    "    # Saturation overrides\n",
    "    #et1.saturation.override_saturation = 'myfile.fits'\n",
    "    \n",
    "    # Reset overrides\n",
    "    #det1.reset.override_reset = 'myfile.fits'\n",
    "        \n",
    "    # Linearity overrides\n",
    "    #det1.linearity.override_linearity = 'myfile.fits'\n",
    "\n",
    "    # RSCD overrides\n",
    "    #det1.rscd.override_rscd = 'myfile.fits'\n",
    "        \n",
    "    # DARK overrides\n",
    "    #det1.dark_current.override_dark = 'myfile.fits'\n",
    "        \n",
    "    # GAIN overrides\n",
    "    #det1.jump.override_gain = 'myfile.fits'\n",
    "    #det1.ramp_fit.override_gain = 'myfile.fits'\n",
    "                \n",
    "    # READNOISE overrides\n",
    "    #det1.jump.override_readnoise = 'myfile.fits'\n",
    "    #det1.ramp_fit.override_readnoise = 'myfile.fits'\n",
    "        \n",
    "    det1.save_results = True # Save the final resulting _rate.fits files\n",
    "    det1(filename) # Run the pipeline on an input list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f129790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the science observation\n",
    "sstring = input_dir + 'jw*mirifu*uncal.fits'\n",
    "lvl1b_files = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(lvl1b_files))\n",
    "    for ii in range(0,len(lvl1b_files)):\n",
    "        hdu=fits.open(lvl1b_files[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    lvl1b_files=lvl1b_files[indx]\n",
    "\n",
    "print('Found ' + str(len(lvl1b_files)) + ' science input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d413253-241c-4d1d-86f7-3256ed03e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_dir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing for SCI data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc3eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look for input files of the form *uncal.fits from the background observation\n",
    "sstring = input_bgdir + 'jw*mirifu*uncal.fits'\n",
    "\n",
    "lvl1b_files = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(lvl1b_files))\n",
    "    for ii in range(0,len(lvl1b_files)):\n",
    "        hdu=fits.open(lvl1b_files[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    lvl1b_files=lvl1b_files[indx]\n",
    "\n",
    "print('Found ' + str(len(lvl1b_files)) + ' background input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea24497-84f3-4a06-b3d9-caa92626af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "if dodet1bg:\n",
    "    for file in lvl1b_files:\n",
    "        rundet1(file, det1_bgdir)\n",
    "else:\n",
    "    print('Skipping Detector1 processing for BG data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41541b35",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419572",
   "metadata": {},
   "source": [
    "4.<font color='white'>-</font>Spec2 Pipeline <a class=\"anchor\" id=\"spec2\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through the Spec2 pipeline in order to produce Lvl2b data products (i.e., calibrated slope images and quick-look data cubes and 1d spectra).  \n",
    "    \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>\n",
    "\n",
    "During stage 2 of the pipeline, the countrate (slope) image products from stage 1, which have units of DN/s, are converted to units of surface brightness (MJy/sr) for both extended and point sources (as of DMS build 9.3/CAL_VER 1.10.2). \n",
    "\n",
    "If Pixel based background subtraction was chosen, this will be applied during Spec2. Doing the Pixel based background subtraction requires an association file to be created to associate the background files with individual science files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c120d056-7135-4977-a48e-0c69306ef0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec2 = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b44e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl2 association file\n",
    "# Requires *one* input sci file, but can have multiple input bg file\n",
    "# The background will not be applied properly to all files if more than *one* sci file is included in the association.\n",
    "def writel2asn(onescifile, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list([onescifile], rule=DMSLevel2bBase, product_name=prodname) # Wrap in array since input was single exposure\n",
    "    \n",
    "    # Channel/band configuration for this sci file\n",
    "    hdu=fits.open(onescifile)\n",
    "    hdr=hdu[0].header\n",
    "    this_channel, this_band = hdr['CHANNEL'], hdr['BAND']\n",
    "    hdu.close()\n",
    "    \n",
    "    # If backgrounds were provided, find which are appropriate to this channel/band and add to association\n",
    "    nbg=len(bgfiles)\n",
    "    if (nbg > 0):\n",
    "        for ii in range(0,nbg):\n",
    "            hdu=fits.open(bgfiles[ii])\n",
    "            hdr=hdu[0].header\n",
    "            if ((hdr['CHANNEL'] == this_channel)&(hdr['BAND'] == this_band)):\n",
    "                asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "            hdu.close()\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d98d0-14e8-47f5-99ad-5d3e75aac3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97390df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "sstring = det1_dir + 'jw*mirifu*rate.fits' #use files from run_folder\n",
    "ratefiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these sci files are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(ratefiles))\n",
    "    for ii in range(0,len(ratefiles)):\n",
    "        hdu=fits.open(ratefiles[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    ratefiles=ratefiles[indx]\n",
    "    \n",
    "# Background Files\n",
    "sstring = det1_bgdir + 'jw*mirifu*rate.fits' \n",
    "bgfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these bg files are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(bgfiles))\n",
    "    for ii in range(0,len(bgfiles)):\n",
    "        hdu=fits.open(bgfiles[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    bgfiles=bgfiles[indx]\n",
    "\n",
    "print('Found ' + str(len(ratefiles)) + ' science files')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec2 pipeline with our desired set of parameters\n",
    "# We'll list the individual steps just to make it clear what's running\n",
    "def runspec2(filename, outdir, nocubes=False):\n",
    "    print(filename)\n",
    "    # Set default configuration from CRDS param reference files.\n",
    "    # -This is required when running the pipeline in a function.\n",
    "    crds_config = Spec2Pipeline.get_config_from_reference(filename)\n",
    "    spec2 = Spec2Pipeline.from_config_section(crds_config)\n",
    "    \n",
    "    spec2.output_dir = outdir\n",
    "\n",
    "    # Assign_wcs overrides\n",
    "    #spec2.assign_wcs.override_distortion = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_regions = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_specwcs = 'myfile.asdf'\n",
    "    #spec2.assign_wcs.override_wavelengthrange = 'myfile.asdf'\n",
    "    \n",
    "    # Background overrides were set up above\n",
    "    if (pixel_bg == True):\n",
    "        spec2.bkg_subtract.skip = False\n",
    "    else:\n",
    "        spec2.bkg_subtract.skip = True\n",
    "\n",
    "    # Flatfield overrides\n",
    "    #spec2.flat_field.override_flat = 'myfile.fits'\n",
    "        \n",
    "    # Straylight overrides\n",
    "    #spec2.straylight.override_mrsxartcorr = 'myfile.fits'\n",
    "        \n",
    "    # Fringe overrides\n",
    "    #spec2.fringe.override_fringe = 'myfile.fits'\n",
    "    \n",
    "    # Photom overrides\n",
    "    #spec2.photom.override_photom = 'myfile.fits'\n",
    "\n",
    "    # Cubepar overrides\n",
    "    #spec2.cube_build.override_cubepar = 'myfile.fits'\n",
    "        \n",
    "    # Extract1D overrides\n",
    "    #spec2.extract_1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec2.extract_1d.override_apcorr = 'myfile.asdf'\n",
    "        \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #spec2.assign_wcs.skip = False\n",
    "    #spec2.bkg_subtract.skip = True\n",
    "    #spec2.flat_field.skip = False\n",
    "    #spec2.srctype.skip = False\n",
    "    #spec2.straylight.skip = False\n",
    "    #spec2.fringe.skip = False\n",
    "    #spec2.photom.skip = False\n",
    "    #spec2.residual_fringe.skip = False #does a residual fringe correction across the entire cube\n",
    "    #spec2.cube_build.skip = False\n",
    "    #spec2.extract_1d.skip = False\n",
    "    \n",
    "    # Run pixel replacement code to extrapolate values for otherwise bad pixels.\n",
    "    # This can help mitigate small 5-10% negative dips in spectra of bright sources.\n",
    "    spec2.pixel_replace.skip=False\n",
    "    spec2.pixel_replace.algorithm='mingrad'\n",
    "    \n",
    "    # This nocubes option allows us to skip the cube building and 1d spectral extraction for individual\n",
    "    # science data frames, but run it for the background data (as the 1d spectra are needed later\n",
    "    # for the master background step in Spec3)\n",
    "    if (nocubes):\n",
    "        spec2.cube_build.skip = True\n",
    "        spec2.extract_1d.skip = True\n",
    "    \n",
    "    # Some cube building options\n",
    "    #spec2.cube_build.weighting='drizzle'\n",
    "    #spec2.cube_build.coord_system='ifualign' # If aligning cubes with IFU axes instead of sky\n",
    "      \n",
    "    spec2.save_results = True\n",
    "    spec2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3628d5a8-7186-4da4-9b38-44c2629f0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through each of the science files, using relevant associated backgrounds in spec2 processing\n",
    "# The background files are used in this step to perform pixel-based background subtraction (if desired) \n",
    "# Otherwise Background subtraction is done later with Spec3 files\n",
    "if dospec2:\n",
    "    for file in ratefiles:\n",
    "        asnfile=os.path.join(output_dir, 'l2asn.json')\n",
    "        writel2asn(file, bgfiles, asnfile, 'Level2')\n",
    "        runspec2(asnfile, spec2_dir, nocubes=True)\n",
    "else:\n",
    "    print('Skipping Spec2 processing for SCI data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd1892-786d-4180-84fe-b5f220b023b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reduce the backgrounds individually\n",
    "# This will be needed for the Masterbackgroud step in Spec3. You can skip this if doing Spec2 pixel based background.\n",
    "if dospec2bg:\n",
    "    for file in bgfiles:\n",
    "        runspec2(file, spec2_bgdir)\n",
    "else:\n",
    "    print('Skipping Spec2 processing for BG data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa83c83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec2: {time1 - time_spec2} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3c6415",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68262d04-b6ca-4dea-b960-b5f744d4baa9",
   "metadata": {},
   "source": [
    "5.<font color='white'>-</font>Spec3 Pipeline: Default configuration<a class=\"anchor\" id=\"spec3\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here we'll run the Spec3 pipeline to produce a composite data cube from all dithered exposures.\n",
    "We will need to create an association file from all science and background data in order for the pipeline to use them appropriately.\n",
    "\n",
    "A word of caution: the data cubes created by the JWST pipeline are in SURFACE BRIGHTNESS units (MJy/steradian), not flux units.  What that means is that if you intend to sum spectra within an aperture you need to be sure to multiply by the pixel area in steradians first in order to get a spectrum in flux units (the PIXAR_SR keyword can be found in the SCI extension header).  This correction is already build into the pipeline Extract1D algorithm.\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html\n",
    "    \n",
    "</div>\n",
    "\n",
    "Spec3 requires an association file to be created to associate the individual _cal.fits science exposures together to be combined into a final combined data cube.\n",
    "<br>\n",
    "- If Master background subtraction was chosen, this will be applied during Spec3. Doing the Background subtraction requires the association file to be created to associate the background _x1d.fits files with all science files. For Master Background subtraction the association file must contain all _x1d.fits background files created with Spec2 and all _cal.fits science files to be combined into the final background subtracted science cube. \n",
    "<br>\n",
    "\n",
    "The extract_1d step is controlled by a different set of parameters in the EXTRACT1D reference file.\n",
    "<br>\n",
    "For point sources:\n",
    ">For point sources a circular extraction aperture is used, along with an optional circular annulus for background extraction and subtraction. The size of the extraction region and the background annulus size varies with wavelength. The extraction related vectors are found in the asdf extract1d reference file. [More Info ...](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html)\n",
    "\n",
    "For extended targets:\n",
    "> For an extended source, rectangular aperture photometry is used, with the entire image being extracted, and no background subtraction, regardless of what was specified in the reference file or step arguments. [More Info ...](https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/description.html)\n",
    "> \n",
    "> Note: Since the Field-of-view for the MRS changes between the different channels, the extraction region size will be difference for the different channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b245a-070d-4e86-ad22-5c62ee35ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spec3 = time.perf_counter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683839d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a useful function to write out a Lvl3 association file from an input list\n",
    "# Note that any background exposures have to be of type x1d.\n",
    "def writel3asn(scifiles, bgfiles, asnfile, prodname):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    # Add background files to the association\n",
    "    nbg=len(bgfiles)\n",
    "    for ii in range(0,nbg):\n",
    "        asn['products'][0]['members'].append({'expname': bgfiles[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519d512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "# Science Files need the cal.fits files\n",
    "sstring = spec2_dir + 'jw*mirifu*cal.fits'\n",
    "calfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "# If desired, check that these are the band/channel to use\n",
    "if ((use_ch != '')&(use_band != '')):\n",
    "    keep=np.zeros(len(calfiles))\n",
    "    for ii in range(0,len(calfiles)):\n",
    "        hdu=fits.open(calfiles[ii])\n",
    "        hdr=hdu[0].header\n",
    "        if ((hdr['CHANNEL'] == use_ch)&(hdr['BAND'] == use_band)):\n",
    "            keep[ii]=1\n",
    "        hdu.close()\n",
    "    indx=np.where(keep == 1)\n",
    "    calfiles=calfiles[indx]\n",
    "\n",
    "# Background Files need the x1d.fits files for Master Background subtraction\n",
    "sstring = spec2_bgdir + 'jw*mirifu*x1d.fits'\n",
    "bgfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "print('Found ' + str(len(bgfiles)) + ' background files to process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an association file that includes all of the different exposures.\n",
    "# _cal.fits science files and _x1d.fits files if doing Master Background subtraction\n",
    "asnfile=os.path.join(output_dir, 'l3asn.json')\n",
    "if dospec3:\n",
    "    writel3asn(calfiles, bgfiles, asnfile, 'Level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c035f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec3 pipeline with our desired set of parameters\n",
    "# This is designed to run on an association file\n",
    "def runspec3(filename):\n",
    "    # Set default configuration from CRDS param reference files\n",
    "    # -This is required when running the pipeline in a function.\n",
    "    crds_config = Spec3Pipeline.get_config_from_reference(filename)\n",
    "    spec3 = Spec3Pipeline.from_config_section(crds_config)\n",
    "    \n",
    "    spec3.output_dir = spec3_dir\n",
    "    spec3.save_results = True\n",
    "    \n",
    "    # Background overrides were set up above\n",
    "    if (master_bg == True):\n",
    "        spec3.master_background.skip = False\n",
    "    else:\n",
    "        spec3.master_background.skip = True\n",
    "    \n",
    "    # Overrides for whether or not certain other steps should be skipped\n",
    "    #spec3.assign_mtwcs.skip = False\n",
    "    spec3.outlier_detection.skip = False\n",
    "    spec3.outlier_detection.kernel_size = '11 1'\n",
    "    spec3.outlier_detection.threshold_percent = 99.5 # Dial this threshold if necessary to be more/less aggressive in outlier flagging\n",
    "    #spec3.mrs_imatch.skip = True\n",
    "    #spec3.cube_build.skip = True\n",
    "    #spec3.extract_1d.skip = False\n",
    "    \n",
    "    # Cube building configuration options\n",
    "    #spec3.cube_build.override_cubepar = 'myfile.fits' # Cubepar parameter override\n",
    "    #spec3.cube_build.output_file = 'mycube' # Custom output name\n",
    "    spec3.cube_build.output_type = 'band' # 'band', 'channel', or 'multi' type cube output\n",
    "    #spec3.cube_build.channel = '1' # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "    #spec3.cube_build.weighting = 'drizzle' # 'emsm' or 'drizzle'\n",
    "    #spec3.cube_build.coord_system = 'ifualign' # 'ifualign', 'skyalign', or 'internal_cal'\n",
    "    #spec3.cube_build.scalexy = 0.5 # Output cube spaxel scale (arcsec) if setting it by hand\n",
    "    #spec3.cube_build.scalew = 0.002 # Output cube voxel depth in wavelength if setting it by hand\n",
    "    #spec3.cube_build.ra_center = 65.0 # Force cube to be centered at this R.A.\n",
    "    #spec3.cube_build.dec_center = -35.0 # Force cube to be centered at this Decl.\n",
    "    #spec3.cube_build.cube_pa = 45.0 # Force cube to have this position angle\n",
    "    #spec3.cube_build.nspax_x = 61 # Require this number of spaxels in cube X direction\n",
    "    #spec3.cube_build.nspax_y = 61 # Require this number of spaxels in cube Y direction\n",
    "    #spec3.cube_build.wavemin = 4.8 # Custom minimum wavelength for the cube\n",
    "    #spec3.cube_build.wavemax = 6.3 # Custom maximum wavelength for the cube\n",
    "        \n",
    "    # Extract1D overrides and config options\n",
    "    #spec3.extract_1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec3.extract_1d.override_apcorr = 'myfile.asdf'\n",
    "    #spec3.extract_1d.ifu_set_srctype = 'POINT' # Force a certain type of spectral extraction\n",
    "    #spec3.extract_1d.ifu_rscale = 2 # Number of FWHM to use for point-source aperture extraction radius (default is 2)\n",
    "    spec3.extract_1d.ifu_autocen = True # Enable auto-centering of the extraction aperture\n",
    "    #spec3.extract_1d.center_xy=(20,20) # Override aperture location if desired\n",
    "    spec3.extract_1d.ifu_rfcorr = True # Turn on 1d residual fringe correction\n",
    "\n",
    "    spec3(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3529218-7de7-4ae8-bb2c-11fffd9f4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dospec3:\n",
    "    runspec3(asnfile)\n",
    "else:\n",
    "    print('Skipping Spec3 processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")\n",
    "print(f\"Runtime for Spec3: {time1 - time_spec3} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1426db95",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966d5ff",
   "metadata": {},
   "source": [
    "6.<font color='white'>-</font>Plot the spectra<a class=\"anchor\" id=\"plots\"></a>\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here we'll plot the spectra to see what our source looks like.\n",
    "    \n",
    "For MIRI-MRS, all extract-1d files are in Flux units of Jy\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf94f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and sort all of the input files\n",
    "\n",
    "# Science Files \n",
    "# Use the finale extracted spectra (x1d.fits) files \n",
    "sstring = spec3_dir + '*x1d.fits'\n",
    "x1dfiles = np.array(sorted(glob.glob(sstring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ae6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal plots\n",
    "%matplotlib inline\n",
    "# Interactive plots\n",
    "#%matplotlib notebook\n",
    "\n",
    "rc('axes', linewidth=2)\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,3), dpi=150)\n",
    "\n",
    "if (len(x1dfiles) > 0):\n",
    "    hdu=fits.open(x1dfiles[0])\n",
    "    objname=hdu[0].header['TARGPROP']\n",
    "else:\n",
    "    objname='Unknown'\n",
    "\n",
    "for file in x1dfiles:\n",
    "    x1d=fits.open(file)\n",
    "    x1ddata=x1d[1].data\n",
    "    wave=x1ddata['WAVELENGTH']\n",
    "    flux=x1ddata['FLUX']\n",
    "    \n",
    "    # labels\n",
    "    label=x1d[0].header['CHANNEL']+x1d[0].header['BAND']\n",
    "    \n",
    "    plt.plot(wave,flux,label=label)\n",
    "    \n",
    "plt.xlabel(r'Wavelength ($\\mu$m)')\n",
    "plt.ylabel('Flux (Jy)')\n",
    "plt.title(objname)\n",
    "plt.ylim(0,0.2)\n",
    "plt.legend(fontsize=8, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877d4af2-2f8b-4b0d-8b1f-58a73818ce6b",
   "metadata": {},
   "source": [
    "7.<font color='white'>-</font>More Advice<a class=\"anchor\" id=\"plots\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898c4b1-cb3d-4581-bf9d-2b9dfa045ed9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "**Warning:** Plotting spectra from only one Spaxel will have sinusoidal variations from resampling noise due to undersampling of the PSF. We therefore recommend always taking a larger aperture for science analysis.  \n",
    "See: https://jwst-docs.stsci.edu/jwst-calibration-pipeline-caveats/jwst-miri-mrs-pipeline-caveats#JWSTMIRIMRSPipelineCaveats-Resamplingnoise\n",
    "<br>\n",
    "<br>\n",
    "Fringing is a spectral artifact that produces periodic amplitude modulations in the continuum in the extracted spectra in MIRI-MRS data but can be corrected across the entire cube using the residual_fringe_correction step in Spec2 or doing a 1D residual fringe correction on just the extracted spectrum with Extract1D. \n",
    "<br>\n",
    "    \n",
    "For details on Fringing see: https://jwst-docs.stsci.edu/jwst-calibration-pipeline-caveats/jwst-miri-mrs-pipeline-caveats#JWSTMIRIMRSPipelineCaveats-Fringing \n",
    "<br>\n",
    "and for details on 1D extraction see: https://jwst-pipeline.readthedocs.io/en/latest/jwst/extract_1d/index.html\n",
    "    \n",
    "\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56da0d-3be6-4046-a0be-44a89ed9748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e2553",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d16ec",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"https://www.stsci.edu/~dlaw/stsci_logo.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
