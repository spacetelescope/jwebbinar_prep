{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e959a152",
   "metadata": {},
   "source": [
    "# How to run the pipeline on a set of imager data and create and subtract a sky background\n",
    "\n",
    "If you have a set of imager data without extended sources such as a nebula or extended galaxy structure, you can create a mean (or median) background image from the data itself and subtract it off after the level two imaging pipeline stage is run to remove any remaining structure from imperfect flat fields or other issues. MIRI images have higher backgrounds at longer wavelengths, so this will be more useful for longer wavelength images, but can be used for any filter, so long as there is no extended source. \n",
    "\n",
    "This example notebook will demonstrate how to process a data set through the stage one and two imager pipelines, then use the cal.fits files to create the background sky and subtract it from each of the individual science images. Once you have the background subtracted images, the different dithers are then combined into a single image in the level 3 imaging pipeline.\n",
    "\n",
    "The code used for the creation and subtraction of the background sky image was created by Karl Gordon and can be found separately on this github page: https://github.com/STScI-MIRI/Imaging_ExampleNB\n",
    "\n",
    "The pipeline documentation can be found here: https://jwst-pipeline.readthedocs.io/en/latest/\n",
    "\n",
    "The pipeline code and install directions are available on GitHub: https://github.com/spacetelescope/jwst\n",
    "\n",
    "The steps in this notebook are:\n",
    "\n",
    "\n",
    "  1)  Read in list of uncalibrated data (uncal.fits) files.\n",
    "\n",
    "  2)  Process through calwebb_detector1.\n",
    "\n",
    "  3)  Process ramp fit (rate.fits) files through calwebb_image2.\n",
    "    \n",
    "  4)  Create and remove a mean sky background from each calibrated (cal.fits) image.\n",
    "\n",
    "  5)  Create an assocation file for the calibrated files.\n",
    "\n",
    "  6)  Run the calibrated files through calwebb_image3 using the association file.\n",
    "    \n",
    "Setup needed before getting started.\n",
    "\n",
    "* Place notebook and data into the directory where you wish to process your data.\n",
    "\n",
    "* Install pipeline (and pip install any missing modules you encounter when you try to run).\n",
    "\n",
    "\n",
    "### Caveats: This method of creating a background to subtract from the data should only be used in fields without extended sources such as galaxy or nebula emission that stretches across the image. It also will not work well with only a couple of dithers. The more dithers you have, the better this will work. Please examine your data before and after, and examine the output sky image to determine whether this does or does not work for your data specifically.\n",
    "\n",
    "This notebook was written in July 2023, and was meant to work with pipeline version 1.11.1. Future versions of the pipeline should work, but backward compatibility of notebooks is not guaranteed with all python package updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb174ce",
   "metadata": {},
   "source": [
    "## Information on Running the pipeline\n",
    "\n",
    "This notebook utilizes the .call() method of running the notebook in python. With this method, the pipeline will retrieve and use any parameter reference file that applies to your data. These files will set certain parts of the code to run or be skipped, or select any parameters that the instrument teams have put as preferred defaults for most science cases. If you run with this method and do not set any custom parameters, you will replicate what you would get from the automated pipeline and populated in MAST. \n",
    "\n",
    "If you wish to customize the parameters, you can read the documentation in the ReadTheDocs link posted above. Each step in the pipeline has a section in the documentation that explains the parameters relevant to that step.  To set the parameters, a configuration dictionary is set up (and shown in the notebook as examples), then that custom configuration overrides the default values that would have come from the parameter reference files.\n",
    "\n",
    "Most parameters that would need customization come in the stage three pipeline, but the jump step in the first pipeline stage, calwebb_detector1, also has a more extensive set of customizable parameters, and reading the documentation for this step could prove useful.\n",
    "\n",
    "In order to run the pipeline and retrieve the necessary reference files from CRDS, there are a couple of CRDS variables that need to be set. CRDS_PATH tells the pipeline where to save and retrieve reference files. This should be set to a local directory where new reference files will be downloaded, and frequently used reference files can be retrieved and used without needing to pull large files across your internet connection. The other path that needs to be set is the CRDS_SERVER_URL, to know where to look to retrieve any new reference files. That path should be set to https://jwst-crds-stsci.edu. In this notebook, these are set using the os.environ statements below.\n",
    "\n",
    "NOTE: you should store only CRDS reference files in your CRDS cache directory. This is in case you need to delete and redo your CRDS cache area at any point, and you don't want to chance erasing anything that should not be deleted.\n",
    "\n",
    "os.environ['CRDS_PATH'] = '/local_crds_path/crds_cache_ops/'\n",
    "\n",
    "os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds-stsci.edu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f32fe1",
   "metadata": {},
   "source": [
    "### Using datamodels\n",
    "\n",
    "This notebook will show how to use datamodels in working with your data. \n",
    "The basic steps involve reading a file into a datamodel type and then opening the individual named extensions as needed.\n",
    "\n",
    "Header data is stored in the .meta extension.\n",
    "\n",
    "Science data is stored in the .data extension.\n",
    "\n",
    "Data quality flags can be accessed with .groupdq (in RampModel data files that allow accessing dq flags of each group in the data) or .dq (in ImageModel data files after calwebb_detector1 is run).\n",
    "\n",
    "For more in-depth information on datamodels and how to use them: https://stdatamodels.readthedocs.io/en/latest/jwst/datamodels/index.html#data-models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858095b7",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "Import modules that will be needed to read in data, run the pipeline and display any plots or visualizations.\n",
    "\n",
    "Packages to be sure are installed:\n",
    "* jwst\n",
    "* regions\n",
    "* jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36c09e",
   "metadata": {},
   "source": [
    "The CRDS path information needs to be set before importing any crds or jwst packages. The jwst installation instructions here: https://jwst-pipeline.readthedocs.io/en/latest/getting_started/quickstart.html show how to set up the CRDS information in a setup file (such as bash.profile or other setup file), but if the paths are not set in a setup file, they can be set in a notebook using os.environ as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff7dd3-946f-4840-adea-609435d4aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes for the Science Platform environment\n",
    "# Preparing cached data\n",
    "import os\n",
    "import glob\n",
    "\n",
    "preloaded_fits_dir = \"/home/shared/preloaded-fits/jwebbinar_31/miri/\"\n",
    "for filename in glob.glob(os.path.join(preloaded_fits_dir, \"*.fits\")):\n",
    "    basename = os.path.basename(filename)\n",
    "    if not os.path.exists(basename):\n",
    "        os.symlink(filename, basename)\n",
    "        \n",
    "# Installing regions from pypi\n",
    "!pip install regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550637e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CRDS path info\n",
    "import os\n",
    "\n",
    "os.environ['CRDS_PATH'] = os.environ['HOME']+'/crds_cache/' \n",
    "os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "print('CRDS cache location: {}'.format(os.environ['CRDS_PATH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import jwst pipeline modules\n",
    "import jwst\n",
    "from jwst.pipeline import Detector1Pipeline, Image2Pipeline, Image3Pipeline # pipeline modules\n",
    "from jwst import datamodels\n",
    "from jwst.datamodels import RampModel, ImageModel, dqflags # Data models and dq values\n",
    "\n",
    "# Needed for associations\n",
    "from jwst import associations\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.associations import asn_from_list\n",
    "\n",
    "# Other modules/functions to work with and examine data\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from astropy.io import ascii, fits\n",
    "from astropy.visualization import simple_norm\n",
    "from astropy.modeling import models, fitting\n",
    "\n",
    "from astropy.stats import sigma_clipped_stats\n",
    "from astropy.convolution import Gaussian1DKernel, convolve\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "from regions import Regions\n",
    "from tweakwcs import JWSTgWCS\n",
    "\n",
    "# Box download imports \n",
    "from astropy.utils.data import download_file\n",
    "from pathlib import Path\n",
    "from shutil import move\n",
    "from os.path import splitext\n",
    "\n",
    "import crds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d276558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the version of the JWST pipeline.\n",
    "\n",
    "print(jwst.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0df1e",
   "metadata": {},
   "source": [
    "### Load helper scripts for sky creation and subtraction\n",
    "\n",
    "Load scripts to create and subtract the mean sky background image, as well as those for displaying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to set for the make_sky module\n",
    "\n",
    "# allow for varying background\n",
    "scalebkg = False\n",
    "\n",
    "# experimental options for additional exclusion\n",
    "exclude_above = None  # exclude all regions above the provided value in MJy/sr\n",
    "exclude_delta = None  # exclude all regions above bkg+exclude_delta given in MJy/sr\n",
    "ds9region = None  # provide a ds9 region file of regions to exclude\n",
    "\n",
    "# display range for images - just for image display\n",
    "drange_cal = [4., 10.]\n",
    "drange_ssub = [-0.1, 5.0]\n",
    "dmap = \"afmhot\"  # same as ds9 bb\n",
    "\n",
    "\n",
    "# Parameter needed for calwebb_image3 output\n",
    "rotation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to create the background image and subtract it from all input cal images\n",
    "# The combination method can be changed between mean or median further down in this code \n",
    "\n",
    "\n",
    "def make_sky(\n",
    "    files,\n",
    "    subfiles=None,\n",
    "    scalebkg=False,\n",
    "    exclude_above=None,\n",
    "    exclude_delta=None,\n",
    "    ds9regions=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make sky background by sigma clipping in image coordinates and subtract it\n",
    "    from all the input files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : strs\n",
    "       Array of cal files to use to create the sky image\n",
    "    subfiles : str\n",
    "       Array of files to subtract sky from. If None, sky will be subracted from all input images. [default=None]\n",
    "    scalebkg : boolean\n",
    "       Scale each image by its median to the average value [default=False]\n",
    "    exclude_above : float\n",
    "       Exclude data above this value from the sky creation\n",
    "    exclude_delta : float\n",
    "       Exclude data above the median bkg + this value from sky creation\n",
    "    ds9regions : ds9 region file\n",
    "       Exclude pixels inside ds9 regions from sky creation\n",
    "    \"\"\"\n",
    "    if ds9regions is not None:\n",
    "        ereg = Regions.read(ds9regions, format=\"ds9\")\n",
    "        # for creg in ereg:\n",
    "        #     creg.radius *= 0.5\n",
    "\n",
    "    istack = None\n",
    "    for k, cfile in enumerate(files):\n",
    "        print(f\"processing {cfile}\")\n",
    "        cdata = datamodels.open(cfile)\n",
    "        if istack is None:\n",
    "            isize = cdata.data.shape\n",
    "            istack = np.empty((isize[0], isize[1], len(files)))\n",
    "            istackmed = np.empty((len(files)))\n",
    "        tdata = cdata.data\n",
    "\n",
    "        # remove all the non imager data\n",
    "        # bdata = cdata.dq & dqflags.pixel[\"DO_NOT_USE\"] > 0\n",
    "        # tdata[bdata] = np.NaN\n",
    "\n",
    "        if exclude_above is not None:\n",
    "            tdata[tdata > exclude_above] = np.NaN\n",
    "\n",
    "        if ds9regions is not None:\n",
    "   \n",
    "\n",
    "            fits_header, fits_hdulist = cdata.meta.wcs.to_fits()\n",
    "            cwcs = WCS(fits_header)  # <-- \"astropy\" wcs\n",
    "\n",
    "            pixx = np.arange(isize[1])\n",
    "            pixy = np.arange(isize[0])\n",
    "            imagex, imagey = np.meshgrid(pixx, pixy)\n",
    "            imagera, imagedec = cwcs.wcs_pix2world(imagex, imagey, 0)\n",
    "            # imagera, imagedec = cwcs.pixel_to_world(imagex, imagey, 0)\n",
    "            skycoord = SkyCoord(imagera, imagedec, unit=\"deg\")\n",
    "            for creg in ereg:\n",
    "                inoutimage = creg.contains(skycoord, cwcs)\n",
    "                tdata[inoutimage] = np.NaN\n",
    "            cdata.data = tdata\n",
    "            cdata.write(cfile.replace(\"cal.fits\", \"cal_mask.fits\"))\n",
    "            # fits.writeto(\"test.fits\", inoutimage * 1., overwrite=True)\n",
    "\n",
    "        istackmed[k] = np.nanmedian(tdata)\n",
    "        print(f\"median sky = {istackmed[k]}\")\n",
    "\n",
    "        if exclude_delta is not None:\n",
    "            tdata[tdata > istackmed[k] + exclude_delta] = np.NaN\n",
    "\n",
    "        istack[:, :, k] = tdata\n",
    "\n",
    "    # adjust the levels to the median\n",
    "    # allows for data taken at different times with different backgrounds\n",
    "    \n",
    "    ##############################################\n",
    "    \n",
    "    #medsky = np.mean(istackmed)  ########### This is where the combination can be changed between mean or median\n",
    "    medsky = np.median(istackmed)\n",
    "    \n",
    "    ##############################################\n",
    "    \n",
    "    if scalebkg:\n",
    "        for k in range(len(files)):\n",
    "            istack[:, :, k] += medsky - istackmed[k]\n",
    "            print(k, np.nanmedian(istack[:, :, k]))\n",
    "    else:\n",
    "        print(\"Not scaling individual images to median bkg\")\n",
    "\n",
    "    skyflat_mean, skyflat_median, skyflat_std = sigma_clipped_stats(\n",
    "        istack, sigma_lower=3, sigma_upper=1, axis=2\n",
    "    )\n",
    "\n",
    "    # subtract the sky properly adjusted from the data\n",
    "    if subfiles is None:\n",
    "        subfiles = files\n",
    "    for k, cfile in enumerate(subfiles):\n",
    "        cdata = datamodels.open(cfile)\n",
    "        cdata.data -= skyflat_mean\n",
    "        if scalebkg:\n",
    "            print(cfile, medsky - istackmed[k])\n",
    "            cdata.data += medsky - istackmed[k]\n",
    "        else:\n",
    "            print(cfile)\n",
    "        ndata = np.isnan(cdata.data)\n",
    "        #cdata.data[ndata] = 0.0  # This sets all NaNs to 0\n",
    "        cdata.dq[ndata] = cdata.dq[ndata] & dqflags.pixel[\"DO_NOT_USE\"]\n",
    "        cdata.write(cfile.replace(\"_cal.fits\", \"_skysub_cal.fits\")) # Set to either cal or i2d images\n",
    "\n",
    "    return skyflat_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90397f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper script to plot images\n",
    "\n",
    "from astropy.visualization import ImageNormalize, ManualInterval, SqrtStretch\n",
    "\n",
    "\n",
    "def show_image(\n",
    "    data_2d, vmin, vmax, xpixel=None, ypixel=None, title=None, dmap=\"binary\",\n",
    "):\n",
    "    \"\"\"Function to generate a 2D, log-scaled image of the data,\n",
    "    with an option to highlight a specific pixel (with a red dot).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_2d : numpy.ndarray\n",
    "        Image to be displayed\n",
    "\n",
    "    vmin : float\n",
    "        Minimum signal value to use for scaling\n",
    "\n",
    "    vmax : float\n",
    "        Maximum signal value to use for scaling\n",
    "\n",
    "    xpixel : int\n",
    "        X-coordinate of pixel to highlight\n",
    "\n",
    "    ypixel : int\n",
    "        Y-coordinate of pixel to highlight\n",
    "\n",
    "    title : str\n",
    "        String to use for the plot title\n",
    "    \"\"\"\n",
    "    norm = ImageNormalize(\n",
    "        data_2d, interval=ManualInterval(vmin=vmin, vmax=vmax), stretch=SqrtStretch()\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    im = ax.imshow(data_2d, origin=\"lower\", norm=norm, cmap=plt.get_cmap(dmap))\n",
    "\n",
    "    if xpixel and ypixel:\n",
    "        plt.plot(xpixel, ypixel, marker=\"o\", color=\"red\", label=\"Selected Pixel\")\n",
    "\n",
    "    fig.colorbar(im, label=\"DN\")\n",
    "    plt.xlabel(\"Pixel column\")\n",
    "    plt.ylabel(\"Pixel row\")\n",
    "    if title:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08885d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper script to plot an image and overlay catalog sources\n",
    "\n",
    "def overlay_catalog(\n",
    "    data_2d,\n",
    "    catalog,\n",
    "    flux_limit=0,\n",
    "    vmin=0,\n",
    "    vmax=10,\n",
    "    title=None,\n",
    "    units=\"MJy/str\",\n",
    "    dmap=\"binary\",\n",
    "):\n",
    "    \"\"\"Function to generate a 2D image of the data,\n",
    "    with sources overlaid.\n",
    "\n",
    "    data_2d : numpy.ndarray\n",
    "        2D image to be displayed\n",
    "\n",
    "    catalog : astropy.table.Table\n",
    "        Table of sources\n",
    "\n",
    "    flux_limit : float\n",
    "        Minimum signal threshold to overplot sources from catalog.\n",
    "        Sources below this limit will not be shown on the image.\n",
    "\n",
    "    vmin : float\n",
    "        Minimum signal value to use for scaling\n",
    "\n",
    "    vmax : float\n",
    "        Maximum signal value to use for scaling\n",
    "\n",
    "    title : str\n",
    "        String to use for the plot title\n",
    "\n",
    "    units : str\n",
    "        Units of the data. Used for the annotation in the\n",
    "        color bar\n",
    "    \"\"\"\n",
    "    norm = ImageNormalize(\n",
    "        data_2d, interval=ManualInterval(vmin=vmin, vmax=vmax), stretch=SqrtStretch()\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    im = ax.imshow(data_2d, origin=\"lower\", norm=norm, cmap=plt.get_cmap(dmap))\n",
    "\n",
    "    for row in catalog:\n",
    "        if row[\"aper_total_flux\"].value > flux_limit:\n",
    "            plt.plot(\n",
    "                row[\"xcentroid\"],\n",
    "                row[\"ycentroid\"],\n",
    "                marker=\"o\",\n",
    "                markersize=\"3\",\n",
    "                color=\"red\",\n",
    "            )\n",
    "\n",
    "    plt.xlabel(\"Pixel column\")\n",
    "    plt.ylabel(\"Pixel row\")\n",
    "\n",
    "    fig.colorbar(im, label=units)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(left=0.15)\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e13cd",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "For this notebook, the data is stored in Box and will be loaded into the working directory.\n",
    "You can also just put the notebook and data into a single directory, or include path information as you work with the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset from Box\n",
    "\n",
    "# def get_box_files(file_list):\n",
    "#     for box_url,file_name in file_list:\n",
    "#         if 'https' not in box_url:\n",
    "#             box_url = 'https://stsci.box.com/shared/static/' + box_url\n",
    "#         downloaded_file = download_file(box_url, timeout=600)\n",
    "#         if Path(file_name).suffix == '':\n",
    "#             ext = splitext(box_url)[1]\n",
    "#             file_name += ext\n",
    "#         move(downloaded_file, file_name)\n",
    "\n",
    "\n",
    "# # F2100W data from a calibration program, target BD+60-1753 \n",
    "# file_urls = ['https://stsci.box.com/shared/static/k0xukhkin1rpaczva8cdws0yrs478zvj.fits',\n",
    "#              'https://stsci.box.com/shared/static/j9wd92jcjn2fqd7waptcyfnl9nks1an0.fits',\n",
    "#              'https://stsci.box.com/shared/static/jw2aklrwwxsbzqfghpp57yuvscj6de08.fits',\n",
    "#              'https://stsci.box.com/shared/static/9zetqwlbq3jtp4fv7zpl84eff0w2st37.fits',\n",
    "#              'https://stsci.box.com/shared/static/lw4g22c1pxwfo3h9cw49y88tlc8m2i9b.fits']\n",
    "\n",
    "\n",
    "# files = ['jw01027004001_02105_00001_mirimage_uncal.fits',\n",
    "#          'jw01027004001_02105_00002_mirimage_uncal.fits',\n",
    "#          'jw01027004001_02105_00003_mirimage_uncal.fits',\n",
    "#          'jw01027004001_02105_00004_mirimage_uncal.fits',\n",
    "#          'jw01027-o004_t006_miri_f2100w_i2d.fits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66e8437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run commands to load the data from box into current directory\n",
    "\n",
    "# box_download_list = [(url,name) for url,name in zip(file_urls,files)]\n",
    "\n",
    "\n",
    "# get_box_files(box_download_list)\n",
    "\n",
    "# print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081967bd-c07b-459a-a7b1-0ef8a5b1f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If files area already in the directory with the data, and you only need the list of files for processing, uncomment these lines.\n",
    "\n",
    "files = ['jw01027004001_02105_00001_mirimage_uncal.fits',\n",
    "        'jw01027004001_02105_00002_mirimage_uncal.fits',\n",
    "        'jw01027004001_02105_00003_mirimage_uncal.fits',\n",
    "        'jw01027004001_02105_00004_mirimage_uncal.fits',\n",
    "        'jw01027-o004_t006_miri_f2100w_i2d.fits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95bbf1-6c9f-43ae-9075-da874c26bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncalfiles = files[:-1]\n",
    "print(uncalfiles)  # print list of uncal files only, minus the i2d file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4aa02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To just gather all uncalfiles in a specific folder: Do not do this if you have multiple datasets in a single folder\n",
    "\n",
    "#uncalfiles = glob.glob('*uncal.fits')\n",
    "#print(uncalfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6406f6b0",
   "metadata": {},
   "source": [
    "### Look at your data and header parameters\n",
    "\n",
    "Read your data files into a RampModel data model to examine some header parameters.\n",
    "Read a sample data file into a model to display the last frame in the ramp to look at the scene in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some header parameters from your data\n",
    "\n",
    "print('File name, Instrument, Subarray,  Filter,  Nints,  Ngroups')\n",
    "\n",
    "for file in uncalfiles: \n",
    "\n",
    "    imfile = RampModel(file) # Read files into datamodel\n",
    "\n",
    "    header = imfile.meta # Read the meta data into a variable called 'header'\n",
    "    \n",
    "    # Read in the header keywords\n",
    "    name = header.filename\n",
    "    inst = header.instrument.name\n",
    "    subarray = header.subarray.name\n",
    "    filt = header.instrument.filter\n",
    "    nints = header.exposure.nints\n",
    "    ngroups = header.exposure.ngroups\n",
    "\n",
    "    print(name, inst, subarray, filt, nints, ngroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77155fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to find a specific header value to view, and you know the FITS header keyword, you \n",
    "# can use this code to find the datamodel equivalent of the keyword.\n",
    "\n",
    "imfile.find_fits_keyword('DATE-OBS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56117d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sample data file into a datamodel\n",
    "uncal_im = RampModel(uncalfiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the last frame of one of the data files to visualize the field of view\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.imshow(uncal_im.data[0, -1, :, :], cmap='Greys', origin='lower', vmin=20000,vmax=40000)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76f9b9",
   "metadata": {},
   "source": [
    "## Run calwebb_detector1 on your data\n",
    "\n",
    "Set a few parameters for the jump step (most steps are fine to run with defaults).\n",
    "Loop through and run all files through calwebb_detector1 to get 'rate.fits' files that contain slope fit images.\n",
    "\n",
    "When setting up the pipeline stage to run, using the .call() method requires setting up dictionaries for each of the steps that will have parameters set that are different from the defaults.\n",
    "\n",
    "#### The detector1 pipeline will take the longest amount of runtime in this notebook. If you do not wish to process all of your data over from the start, you can download the cal.fits files from the archive, load those into the notebook, and run the background creation code from that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few Jump step parameters, for versions 1.10.0 and later (build version 9.2)\n",
    "# find_showers turns the cr shower code on for MIRI. Set to False to skip this code. (Default is currently False.)\n",
    "# Use mostly code defaults for the rest of the parameters.\n",
    "\n",
    "rej_thresh = 5\n",
    "\n",
    "expand_large_events = False  # This parameter is used for NIR instruments\n",
    "find_showers = False  # This parameter is used for MIRI only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a few parameters and run each file through the Detector1 Pipeline\n",
    "\n",
    "for file in uncalfiles:\n",
    "    pipe = Detector1Pipeline()\n",
    "    \n",
    "    # set up output file name\n",
    "    base, remainder = file.split('_uncal')\n",
    "    outname = base\n",
    "    \n",
    "    # Set up dictionaries for step parameters\n",
    "    cfg = dict()\n",
    "    cfg['jump'] = {}\n",
    "    cfg['jump']['save_results'] = True # The jump output file is useful if you want to track which frames have jumps flagged\n",
    "    cfg['jump']['rejection_threshold'] = rej_thresh\n",
    "    cfg['jump']['expand_large_events'] = expand_large_events\n",
    "    cfg['jump']['find_showers'] = find_showers\n",
    "    cfg['jump']['output_file'] = base + '.fits'\n",
    "    #cfg['jump']['output_dir'] = datadir\n",
    "\n",
    "    #cfg['ramp_fit'] ={}\n",
    "    #cfg['ramp_fit']['output_file'] = base + '.fits'\n",
    "    #cfg['ramp_fit']['output_dir'] = datadir\n",
    "\n",
    "\n",
    "    pipe.call(file, steps=cfg, save_results=True, output_file =  base + '.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cdf56",
   "metadata": {},
   "source": [
    "#### Look at output rate image\n",
    "\n",
    "Plot one of the output rate images to see what the image looks like. The code here will also mark any pixels with a DQ value of DO_NOT_USE in blue. These blue pixels are considered bad in some way and will not contribute to the final combined image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f444490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and look at a single rate image\n",
    "  \n",
    "ratefiles = [ele.replace('uncal', 'rate') for ele in uncalfiles]\n",
    "rate_im = ImageModel(ratefiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df34178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the averaged slope image, and plot the DO_NOT_USE dq flagged pixels in blue\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# mask out DO_NOT_USE values of 1\n",
    "masked_rate_im = np.ma.masked_where((rate_im.dq & dqflags.pixel['DO_NOT_USE'] > 0), rate_im.data)\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap(\"Greys\").copy()  # Can be any colormap that you want after the cm\n",
    "cmap.set_bad(color='blue') # color to mark all DO_NOT_USE pixels\n",
    "\n",
    "plt.imshow(masked_rate_im, cmap=cmap, origin='lower', vmin=500,vmax=580.0)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a92b3",
   "metadata": {},
   "source": [
    "## Run calwebb_detector2\n",
    "\n",
    "Run the output of detector1 (rate files) through calwebb_detector2 to obtain a set of calibrated files (cal files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Calwebb_image2 on output files from detector1    \n",
    "    \n",
    "print('There are ', len(ratefiles), ' images.')\n",
    "    \n",
    "callist = []\n",
    "\n",
    "# cycle through files\n",
    "for im in ratefiles:\n",
    "\n",
    "    calfile = Image2Pipeline.call(im, save_results=True)\n",
    "\n",
    "    callist.append(calfile)\n",
    "\n",
    "print(callist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49890696",
   "metadata": {},
   "source": [
    "## Create and subtract mean sky background image\n",
    "\n",
    "The input for the following steps is the list of calibrated files that were the result of calwebb_image2 processing. These files will be used to create the mean (or median) background image. Once the background image is created, it will be subtracted from each of the individual cal files, creating a set of background subtracted calibrated files. These files can then be processed through the last stage of the pipeline, calwebb_image3, to combine the background subtracted files into a dither combined mosaic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a499ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the filter being used in the data, so that it can be used in output file names.\n",
    "\n",
    "filter = 'F2100W' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the list of cal files for the sky subtraction steps.\n",
    "\n",
    "#miri_cal_files = glob.glob('*cal.fits') # this will collect all cal files in the directory\n",
    "\n",
    "miri_cal_files = [ele.replace('rate', 'cal') for ele in ratefiles]\n",
    "\n",
    "print(miri_cal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the script make_sky to make and subtract off the sky background image.\n",
    "\n",
    "simage = make_sky(miri_cal_files, scalebkg=scalebkg,\n",
    "                  ds9regions=ds9region, exclude_above=exclude_above, exclude_delta=exclude_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09532bdc",
   "metadata": {},
   "source": [
    "### Look at a sample image before and after sky subtraction\n",
    "\n",
    "Also look at the mean sky image itself and write it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9203895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at cal image before sky subtraction\n",
    "\n",
    "im = ImageModel(miri_cal_files[0])\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(im.data, cmap='Greys', origin='lower', vmin=200,vmax=230)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2112a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the created median sky image and write to a file\n",
    "\n",
    "drange_cal = [40., 230.]\n",
    "show_image(simage, drange_cal[0], drange_cal[1], dmap=dmap)\n",
    "fits.writeto(filter+'_sky.fits', simage, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0152b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather sky subtracted cal files\n",
    "\n",
    "miri_skysub_files = [ele.replace('cal', 'skysub_cal') for ele in miri_cal_files]\n",
    "\n",
    "print(miri_skysub_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at cal image after sky subtraction\n",
    "\n",
    "im2 = ImageModel(miri_skysub_files[0])\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(im2.data, cmap='Greys', origin='lower', vmin=0,vmax=1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8561ef0",
   "metadata": {},
   "source": [
    "### Run Calwebb_image3 on background subtracted cal files\n",
    "\n",
    "First create a new association file for the background subtracted files, and then use that file to process the set through calwebb_image3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36314289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use asn_from_list to create association table\n",
    "miri_asn_name = 'miri_'+filter+'_stage3_asn_skysub' # name of output asn file\n",
    "\n",
    "asn = asn_from_list.asn_from_list(miri_skysub_files, rule=DMS_Level3_Base, product_name=miri_asn_name)\n",
    "\n",
    "# dump association table to a .json file for use in image3\n",
    "\n",
    "miri_asn_file = miri_asn_name+'.json'\n",
    "with open(miri_asn_file, 'w') as outfile:\n",
    "    outfile.write(asn.dump()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf9ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run calwebb_image3 (or Image3Pipeline) on sky subtracted data using association table.\n",
    "\n",
    "cfg = dict() \n",
    "\n",
    "cfg['tweakreg'] = {}\n",
    "cfg['tweakreg']['abs_refcat'] = 'GAIADR3'\n",
    "#cfg['skymatch'] = {'skip' : True} \n",
    "\n",
    "cfg['resample']={}  # set up empty dictionary for multiple parameters to be set per step\n",
    "cfg['resample']['rotation'] = rotation\n",
    "cfg['resample']['kernel'] = 'gaussian'\n",
    "cfg['resample']['weight_type'] = 'exptime'\n",
    "\n",
    "#cfg['outlier_detection'] = {'save_intermediate_results' : True}  # Can set single parameters with this syntax\n",
    "#cfg['source_catalog'] = {'deblend': True} # test to see if this crashes. If scikit-image is not installed, this will crash.\n",
    "                     \n",
    "output = Image3Pipeline.call(miri_asn_file, steps=cfg, save_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ab69f",
   "metadata": {},
   "source": [
    "#### Look at the output combined mosaic and overlay the source catalog to see what sources were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8b809-c7e6-4443-be2e-d76802bbc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the final i2d image (combined mosaic)\n",
    "\n",
    "# Read your mosaic image into an ImageModel datamodel\n",
    "miri_mosaic_file =  miri_asn_name + '_i2d.fits'\n",
    "miri_mosaic = ImageModel(miri_mosaic_file)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "# Set up image\n",
    "cax = ax.imshow(miri_mosaic.data, cmap='Greys', origin='lower', vmin=0,vmax=2)\n",
    "\n",
    "# Set up colorbar\n",
    "cb = fig.colorbar(cax)\n",
    "cb.ax.set_ylabel('MJy/str',fontsize=14)\n",
    "\n",
    "#Set labels \n",
    "ax.set_xlabel('X',fontsize=16)\n",
    "ax.set_ylabel('Y',fontsize=16)\n",
    "ax.set_title('Final MIRI mosaic', fontsize=16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220809de-3033-4e17-8962-5e7456c69dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total exposure time of combined mosaic: ',miri_mosaic.meta.resample.product_exposure_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b12b8-47e8-4f4d-ab3c-b16c8f5fd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to display of background subtracted i2d file to non-background subtracted file\n",
    "\n",
    "# read in non-background subtracted file\n",
    "default_pipeline_mosaic = ImageModel(files[4])\n",
    "\n",
    "# Set up two plots side by side\n",
    "fig, ax = plt.subplots(2, 1,figsize=[15,15])\n",
    "cmap='Greys'\n",
    "\n",
    "ax[0].set_title('Backround subtracted i2d file')\n",
    "ax[1].set_title('Non-background subtracted i2d file')\n",
    "\n",
    "cset1 = ax[0].imshow(miri_mosaic.data,cmap=cmap, origin='lower', vmin=0, vmax=2)\n",
    "ax[0].set_xlabel('X',fontsize=14)\n",
    "ax[0].set_ylabel('Y',fontsize=14)\n",
    "cb1 = fig.colorbar(cset1, ax=ax[0])\n",
    "cb1.ax.set_ylabel('MJy/str',fontsize=14)\n",
    "\n",
    "\n",
    "cset2 = ax[1].imshow(default_pipeline_mosaic.data,cmap=cmap, origin='lower', vmin=214, vmax=220)\n",
    "ax[1].set_xlabel('X',fontsize=14)\n",
    "ax[1].set_ylabel('Y',fontsize=14)\n",
    "cb2 = fig.colorbar(cset2, ax=ax[1])\n",
    "cb2.ax.set_ylabel('MJy/str',fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b90c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mosaic data and sources found with source_catalog\n",
    "\n",
    "miri_catalog_file = miri_asn_name + '_cat.ecsv'\n",
    "\n",
    "# Read in the source catalog\n",
    "miri_source_cat = ascii.read(miri_catalog_file)\n",
    "\n",
    "# Show the catalog sources on the mosaic\n",
    "overlay_catalog(miri_mosaic.data, miri_source_cat, flux_limit=5e-7, vmin=0, vmax=2,\n",
    "                title='Final MIRI mosaic with source catalog', dmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cf38e-5848-405e-8f3c-828b779a9142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "masterclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
