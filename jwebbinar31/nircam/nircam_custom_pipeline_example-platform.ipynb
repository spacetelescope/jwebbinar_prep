{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4eea115-ccb9-4036-b96b-da516a76aa6a",
   "metadata": {},
   "source": [
    "# NIRCam Custom Pipeline Example\n",
    "---\n",
    "**Author**: Ben Sunnquist (bsunnquist@stsci.edu)| **Latest Update**: 6 Feb 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6212918a-75fc-4252-8e51-b6ff7fd24091",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h3><u><b>Notebook Goals</b></u></h3>  \n",
    "<ul>\n",
    "    <li>Correct snowballs in rate images using custom jump step settings</li>\n",
    "    <li>Manually flag anomalous regions (e.g. persistence, scattered light) in data quality arrays to remove them from the final drizzled image</li>\n",
    "    <li>Correct 1/f noise and amplifier offset residuals</li>\n",
    "    <li>Correct alignment issues in data with few good sources by feeding custom source catalogs into tweakreg</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f7a74-5038-439e-96e0-875e0a06c695",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#intro)\n",
    "* [Pipeline Resources and Documentation](#resources)\n",
    "* [Imports](#imports)\n",
    "* [Convenience Functions](#funcs)\n",
    "* [Data](#data)\n",
    "* [Run the default pipeline](#default_pipeline)\n",
    "* [Check reference files](#reffile_check)\n",
    "* [Run the detector1 pipeline with custom jump step settings for snowballs](#detector1)\n",
    "* [Flag persistence regions in data quality arrays](#persistence)\n",
    "* [Run the image2 pipeline](#image2)\n",
    "* [Correct 1/f residuals and amplifier offsets](#1overf)\n",
    "    * [Create image segmentation maps with blotting](#segmaps)\n",
    "    * [Run the 1/f and amplifier offset correction](#1overf_corr)\n",
    "* [Correct tweakreg alignment issues](#tweakreg)\n",
    "    * [Generate custom source catalogs for tweakreg](#catalogs)\n",
    "    * [Run the image3 pipeline with the custom tweakreg catalogs](#image3)\n",
    "* [Compare the results of the default and custom pipeline runs](#compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff412d90-bab4-4cd6-9dc0-d177c9c49610",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfd874-d880-4b96-bc5c-5782228ef2d0",
   "metadata": {},
   "source": [
    "This notebook will calibrate NIRCam imaging data all the way through the JWST pipeline, from uncalibrated images to a combined final drizzled image. \n",
    "\n",
    "After processing with the default pipeline settings, several issues will be discovered and investigated, including snowballs/asteroids, persistence, 1/f and amplifier offset residuals, and image alignment/source catalog issues. We'll solve each of these issues through a combination of re-running the pipeline steps with custom pipeline settings and manually modifying the images in between pipeline stages. Along the way, we'll leverage several useful pipeline functions to help investigate and correct these issues. \n",
    "\n",
    "At the end of the notebook, we'll compare the final products of the default pipeline run to our custom pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f324a5f-24ab-4579-92e0-793f562dca41",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "## Pipeline Resources and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28b035-86dd-44f5-8aa8-4e69413c6a4a",
   "metadata": {},
   "source": [
    "* [JWST Pipeline Documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/user_documentation/introduction.html) includes descriptions and parameters for all pipeline steps.\n",
    "\n",
    "* [JWST Pipeline Github](https://github.com/spacetelescope/jwst) contains the source code and installation instructions for the pipeline.\n",
    "\n",
    "* [CRDS Reference Files](https://jwst-crds.stsci.edu/) is where all JWST reference files used by the pipeline are located.\n",
    "\n",
    "* Submit a ticket to the [Help Desk](https://stsci.service-now.com/jwst?id=sc_cat_item&sys_id=27a8af2fdbf2220033b55dd5ce9619cd&sysparm_category=e15706fc0a0a0aa7007fc21e1ab70c2f) if you have any questions or problems regarding the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88168582-65e5-47c9-b4a2-b966833b5df3",
   "metadata": {},
   "source": [
    "<a id='imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd872d-8ab4-45ad-813e-4bd42ef2b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changes for the Science Platform environment\n",
    "# Preparing cached data\n",
    "import os\n",
    "import glob\n",
    "\n",
    "preloaded_fits_dir = \"/home/shared/preloaded-fits/jwebbinar_31/nircam/nircam_custom_pipeline_example/\"\n",
    "for filename in glob.glob(os.path.join(preloaded_fits_dir, \"*.fits\")):\n",
    "    basename = os.path.basename(filename)\n",
    "    if not os.path.exists(basename):\n",
    "        os.symlink(filename, basename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78e389-9fc3-414f-8502-3a94f354d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How I made this environment:\n",
    "# conda create -n jwst1125 python=3.11 notebook\n",
    "# source activate jwst1125\n",
    "# pip install jwst==1.12.5\n",
    "\n",
    "import os\n",
    "os.environ['CRDS_PATH'] = '/home/jovyan/crds_cache/' \n",
    "os.environ['CRDS_SERVER_URL'] = 'https://jwst-crds.stsci.edu'\n",
    "\n",
    "from astropy.convolution import convolve, Gaussian2DKernel\n",
    "from astropy.stats import sigma_clipped_stats, sigma_clip\n",
    "from astropy.table import Table\n",
    "from astropy.visualization import ZScaleInterval\n",
    "import astropy.io.fits as fits\n",
    "import glob\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.segmentation import detect_sources, detect_threshold\n",
    "from PIL import Image, ImageDraw\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import crds\n",
    "import jwst\n",
    "from jwst.associations import asn_from_list\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base\n",
    "from jwst.datamodels import dqflags, ImageModel\n",
    "from jwst.outlier_detection.outlier_detection import gwcs_blot\n",
    "from jwst.pipeline import Detector1Pipeline, Image2Pipeline, Image3Pipeline\n",
    "print('Using JWST Pipeline v{}'.format(jwst.__version__))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeea61c-388f-48b9-b451-aaaf6ed6f1a7",
   "metadata": {},
   "source": [
    "<a id='funcs'></a>\n",
    "## Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f2d88-f21b-4d24-882b-95a8a2afa9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ramp_data(file, x, y, cutout=5):\n",
    "    \"\"\"Plots up-the-ramp values and displays the science and\n",
    "    data quality arrays for all groups in the jump image as well\n",
    "    as the rate image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file : str\n",
    "        The rate image file name. A corresponding jump file should be\n",
    "        in the same location.\n",
    "\n",
    "    x : int\n",
    "        The x location of the pixel of interest.\n",
    "\n",
    "    y : int\n",
    "        The y location of the pixel of interest.\n",
    "\n",
    "    cutout : int\n",
    "        The cutout size in pixels around the pixel of interest for\n",
    "        the image displays. The full dimension will be cutout*2 pixels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get relevant data\n",
    "    data = fits.getdata(file, 'SCI')\n",
    "    dq = fits.getdata(file, 'DQ')\n",
    "    ramp_data = fits.getdata(file.replace('rate.fits', 'jump.fits'), 'SCI')\n",
    "    ramp_dq = fits.getdata(file.replace('rate.fits', 'jump.fits'), 'GROUPDQ')\n",
    "    rate = data[y,x]\n",
    "    ramp_vals = ramp_data[:,:,y,x]\n",
    "    groups = np.arange(0, ramp_data.shape[1]).astype(int)\n",
    "    ngroups = len(groups)\n",
    "    \n",
    "    # Plot up-the-ramp signal\n",
    "    fig, ax = plt.subplots(3, ngroups, figsize=(ngroups*4, ngroups*3), constrained_layout=True)\n",
    "    ax[0,0].scatter(groups+1, ramp_vals)\n",
    "    ax[0,0].grid(ls='--')\n",
    "    ax[0,0].set_xlabel('Group #')\n",
    "    ax[0,0].set_ylabel('Signal [DN]')\n",
    "    ax[0,0].set_title('Pixel {}, {}'.format(x,y))\n",
    "    \n",
    "    # Plot rate image and dq\n",
    "    fig.delaxes(ax[0,3])\n",
    "    z = ZScaleInterval()\n",
    "    vmin, vmax = z.get_limits(data)\n",
    "    ax[0,1].imshow(data[y-cutout:y+cutout+1, x-cutout:x+cutout+1], \n",
    "                   vmin=vmin, vmax=vmax, origin='lower', cmap='gray')\n",
    "    ax[0,1].set_title('Rate Image\\n{:.3f} DN/s'.format(data[y,x]))\n",
    "    ax[0,2].imshow(dq[y-cutout:y+cutout+1, x-cutout:x+cutout+1], \n",
    "                   vmin=0, vmax=0.1, origin='lower', cmap='gray')\n",
    "    dq_vals = dqflags.dqflags_to_mnemonics(dq[y,x], dqflags.pixel)\n",
    "    if len(dq_vals)==0:\n",
    "        dq_vals = \"{'GOOD'}\"\n",
    "    ax[0,2].set_title('Rate DQ\\n{}'.format(dq_vals))\n",
    "    \n",
    "    # Plot each group image and dq\n",
    "    for i in groups:\n",
    "        vmin, vmax = z.get_limits(ramp_data[0,i])\n",
    "        ax[1,i].imshow(ramp_data[0,i,y-cutout:y+cutout+1, x-cutout:x+cutout+1], \n",
    "                       vmin=vmin, vmax=vmax, origin='lower', cmap='gray')\n",
    "        ax[1,i].set_title('Group {} Image\\n{} DN'.format(i+1, int(ramp_data[0,i,y,x])))\n",
    "        ax[2,i].imshow(ramp_dq[0,i,y-cutout:y+cutout+1, x-cutout:x+cutout+1], \n",
    "                       vmin=0, vmax=0.1, origin='lower', cmap='gray')\n",
    "        dq_vals = dqflags.dqflags_to_mnemonics(ramp_dq[0,i,y,x], dqflags.pixel)\n",
    "        if len(dq_vals)==0:\n",
    "            dq_vals = \"{'GOOD'}\"\n",
    "        ax[2,i].set_title('Group {} DQ\\n{}'.format(i+1, dq_vals))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2203a09c-2af9-4bd6-9ce6-d5665193d25b",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdcd98-2d6d-4577-8b70-6edb216315e3",
   "metadata": {},
   "source": [
    "We'll use 5 dithered detector B3 images of a sparse calibration field near the ecliptic (PID-4443 observation 2). The data uses a readout pattern of DEEP8 with 4 groups/integration and one integration/exposure. The filter/pupil used are F070W/CLEAR. We'll also use the 5 corresponding detector BLONG F277W/CLEAR calibrated images for source identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d0b19-6289-4faa-a850-510e3c95688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Files used:')\n",
    "for file in sorted(glob.glob('*nrcb3*_uncal.fits')) + sorted(glob.glob('*nrcblong*_cal.fits')):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb073971-b644-4070-bab4-904db8d392c0",
   "metadata": {},
   "source": [
    "<a id='default_pipeline'></a>\n",
    "## Run default pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726fe29-c178-4cdd-9131-55b7f862e671",
   "metadata": {},
   "source": [
    "First we'll run all stages of the pipeline using all default parameters. The only step we'll change is to skip the dark current correction as it takes a very long time to run, and in the shortwave it doesn't change the results much as most pixels in the dark reference file are set to zero; however, this will result in a handful of uncorrected warm/hot pixels, so this isn't recommended for normal processing. We'll output the results of the jump step and tweakreg catalogs, and save all of the output files with the \"_default\" suffix. We'll use these outputs to identify and debug issues and for comparison to the custom pipeline run.\n",
    "\n",
    "[Next Cell](#default_pipeline_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76427070-5a42-4e99-b011-ea8ee89d785f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run all stages of the pipeline using all default parameters (estimated runtime ~8 min)\n",
    "\n",
    "# Run detector1 pipeline\n",
    "uncal_files = sorted(glob.glob('*nrcb3*_uncal.fits'))\n",
    "for file in uncal_files:\n",
    "    result = Detector1Pipeline.call(file, save_results=True,\n",
    "                                    steps={'jump': {'save_results': True},\n",
    "                                           'dark_current': {'skip': True}}, \n",
    "                                    output_file='{}default'.format(os.path.basename(file).split('uncal')[0]))\n",
    "\n",
    "# Run image2 pipeline\n",
    "rate_files = sorted(glob.glob('*nrcb3_default_rate.fits'))\n",
    "for file in rate_files:\n",
    "    result = Image2Pipeline.call(file, save_results=True)\n",
    "\n",
    "# Create association file for image3\n",
    "cal_files = sorted(glob.glob('*nrcb3_default_cal.fits'))\n",
    "asn = asn_from_list.asn_from_list(cal_files, rule=DMS_Level3_Base, product_name='nircam_f070w_default')\n",
    "with open('nircam_f070w_default.json', 'w') as outfile:\n",
    "    outfile.write(asn.dump()[1])\n",
    "\n",
    "# Run image3 pipeline\n",
    "result = Image3Pipeline.call('nircam_f070w_default.json', save_results=True, steps={'tweakreg': {'save_catalogs': True}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e35e3e3-a58d-441a-a2f7-52348a4f9570",
   "metadata": {},
   "source": [
    "<a id='default_pipeline_output'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8e421-638b-4d3a-aa77-9ce37f4eec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the final default drizzle product\n",
    "\n",
    "data = fits.getdata('nircam_f070w_default_i2d.fits', 'SCI')\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(data, origin='lower', cmap='gray', vmin=0.1, vmax=0.5)\n",
    "plt.title('Default Drizzle', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34365fe-66f0-4cfe-ae38-3aac7a692671",
   "metadata": {},
   "source": [
    "Several issues are apparent in the data from the default pipeline run:\n",
    "* Multiple copies of sources are visible, i.e. the data is misaligned\n",
    "* The upper right corners of each image have high signal levels\n",
    "* Strong horizontal noise\n",
    "* Several smaller artifacts with bright halos\n",
    "* Lots of outlier/bad pixels in areas with low coverage\n",
    "\n",
    "For the remainder of this notebook, we'll investigate and correct each of these issues and generate a new final product using custom pipeline processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70d992-f81a-47d5-a582-57e64935a31a",
   "metadata": {},
   "source": [
    "<a id='reffile_check'></a>\n",
    "## Check pipeline reference files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea824089-a489-45fc-bb6d-22b2885fe295",
   "metadata": {},
   "source": [
    "A good first step when investigating issues in your data is confirming the data was calibrated with the most recent references files available in CRDS, as these references files are routinely updated and improved. All reference files used by the pipeline are stored in [CRDS](https://jwst-crds.stsci.edu/), and those used for your specific data are written out while running the pipeline and stored in the data's primary headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d787475-cf84-438c-88af-50720d7c3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that data is using the most recent reference files from CRDS.\n",
    "# This cell can be used with any input file type e.g. rate, cal, i2d.\n",
    "\n",
    "file = 'nircam_f070w_default_i2d.fits'\n",
    "header = fits.getheader(file, 'PRIMARY')\n",
    "reffile_mapping = crds.getrecommendations(header)\n",
    "for reffile in reffile_mapping:\n",
    "    reffile_match = os.path.basename(reffile_mapping[reffile])\n",
    "    if 'n/a' in reffile_mapping[reffile]:  # not all reffiles are relevant to dataset\n",
    "        continue\n",
    "    try:\n",
    "        reffile_used = os.path.basename(header[crds.jwst.locate.filekind_to_keyword(reffile)])\n",
    "        if reffile_used != reffile_match:\n",
    "            print('WARNING: Mismatch for {} reference file: \\n \\t Expected: {} \\n \\t     Used: {}'.format(reffile, \n",
    "                                                                                                          reffile_match, reffile_used))\n",
    "        else:\n",
    "            print('Successfully matched {} reference file: \\n \\t Expected: {} \\n \\t     Used: {}'.format(reffile, \n",
    "                                                                                                         reffile_match, reffile_used))\n",
    "    except KeyError:\n",
    "        print('{} reference file not found in image header.'.format(reffile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036d4c9-70d2-4a20-bbf0-4dffd02da5c4",
   "metadata": {},
   "source": [
    "<a id='detector1'></a>\n",
    "## Run the detector1 pipeline with custom jump step settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13951cb2-d791-4011-b415-d92e5ccb70a4",
   "metadata": {},
   "source": [
    "Let's check the up-the-ramp signal and data quality flags for a couple of the artifacts with bright halos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e72c6-dc40-4df3-87e2-bbd285a399a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ramp_data('jw04443002001_02101_00012_nrcb3_default_rate.fits', 1155, 346, cutout=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12376f4f-8c71-46e5-a79e-e76ecdf94ca7",
   "metadata": {},
   "source": [
    "These objects are [\"Snowballs\"](https://jwst-docs.stsci.edu/data-artifacts-and-features/snowballs-and-shower-artifacts). Snowballs are bright circular sources caused by large cosmic ray impacts that appear on a timescale much shorter than the detector readout time. The cores are often saturated and the full extent of their impact is often missed by the jump step in the pipeline, resulting in a bright halo in the rate images due to these outer pixels being unflagged during the ramp fit step.\n",
    "\n",
    "The jump step in the pipeline offers several parameters to catch and flag the extent of these snowballs (see \"Parameters that affect Near-IR Snowball Flagging\" section in the [jump step arguments page](https://jwst-pipeline.readthedocs.io/en/latest/jwst/jump/arguments.html)). We'll tweak several of these snowball parameters in the custom detector1 pipeline run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dad3f-3a66-4170-a9cf-9f1257ec0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ramp_data('jw04443002001_02101_00004_nrcb3_default_rate.fits', 1895, 185, cutout=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca796035-327c-460e-8118-2aefe49ae9b0",
   "metadata": {},
   "source": [
    "This other bright halo artifact appears to be an asteroid or some other moving object. Similar to the snowballs, the full extent of this object is not flagged in the jump step, resulting in a bright halo visible in the rate image due to the outer wings being unflagged in the ramp fit step. The solution for this artifact will be similar to the snowballs - we'll use custom jump settings in the detector1 pipeline run to expand the flagged regions which avoids the bright wings being included in the ramp fit step ([jump step arguments page](https://jwst-pipeline.readthedocs.io/en/latest/jwst/jump/arguments.html)).\n",
    "\n",
    "[Next Cell](#detector1_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09197af1-1f3d-4799-a4ed-b4791e4d5026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run detector1 pipeline with expanded snowball/asteroid flagging. Expand large jump event dq flags by 2.5x, where\n",
    "# large events are defined as 100 interconnected pixels flagged as a jump (no saturation required).\n",
    "# Similar to the default pipeline run, skip the dark current step to save time.\n",
    "\n",
    "# Run detector1 pipeline\n",
    "uncal_files = sorted(glob.glob('*nrcb3*_uncal.fits'))\n",
    "for file in uncal_files:\n",
    "    result = Detector1Pipeline.call(file, save_results=True, steps={'jump': {'save_results': True, \n",
    "                                                                             'expand_large_events': True, \n",
    "                                                                             'sat_required_snowball': False, \n",
    "                                                                             'min_jump_area': 100, \n",
    "                                                                             'expand_factor': 2.5},\n",
    "                                                                    'dark_current': {'skip': True}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1f3a9-77b7-4e29-aff2-a22e4ad1d00c",
   "metadata": {},
   "source": [
    "<a id='detector1_output'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e47896-12b0-47a6-9c07-dd5924429621",
   "metadata": {},
   "source": [
    "After using these custom jump settings, we see the jump data qualty flags have been expanded to include the full extent of these objects. Since the bright halo areas are now flagged, they aren't included in the ramp fit step and don't show up in the rate image anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f41443-8164-487c-944c-f4e775ec763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ramp_data('jw04443002001_02101_00012_nrcb3_rate.fits', 1155, 346, cutout=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0beb518d-4352-4d03-a528-112e8e3f9070",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ramp_data('jw04443002001_02101_00004_nrcb3_rate.fits', 1895, 185, cutout=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b72cb-29be-4765-8187-f9e075212d7c",
   "metadata": {},
   "source": [
    "<a id='persistence'></a>\n",
    "## Flag persistence regions in data quality arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08c8d5-1a1f-4cba-929a-1aa80ceabaff",
   "metadata": {},
   "source": [
    "The upper right corner of our images show increased signal that decays with time, where the affected region is fixed in detector space. This is characteristic of [persistence](https://jwst-docs.stsci.edu/jwst-near-infrared-camera/nircam-performance/nircam-persistence), which is a residual signal caused by bright sources imaged prior to the affected data. In our case, a main belt asteroid survey covered in bright sources was imaged ~6 hours prior to our data.\n",
    "\n",
    "Several detector areas on NIRCam are especially sensitive to persistence (see these areas [here](https://jwst-docs.stsci.edu/jwst-near-infrared-camera/nircam-performance/nircam-persistence#NIRCamPersistence-Persistencemaps)), including the upper right corner of B3. Large areas on A3 and B4 show similar behavior.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8e198-fb27-475d-9d12-b463a351f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the individual images to view the persistence signal in the upper right corner\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(50,10))\n",
    "files = sorted(glob.glob('*nrcb3_default_cal.fits'))\n",
    "for i,file in enumerate(files):\n",
    "    data = fits.getdata(file, 'SCI')\n",
    "    date, time = fits.getheader(file)['DATE-OBS'], fits.getheader(file)['TIME-OBS']\n",
    "    ax[i].imshow(data, origin='lower', cmap='gray', vmin=0.2, vmax=.5)\n",
    "    ax[i].set_title('{}\\n{}'.format(date, time), fontsize=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384fb2e-3d27-4412-9192-64194e8b672a",
   "metadata": {},
   "source": [
    "Currently, the persistence step in the pipeline doesn't do anything and persistence modeling is still ongoing, so we'll opt to manually flag these areas with high persistene in each image's data quality array before running the remaining pipeline stages. This will prevent these affected areas from being used when creating the final drizzled image. \n",
    "\n",
    "While we're flagging persistence here, this same method can be used to manually flag any other anomalous features in your data, such as the NIRCam scatterd light  [\"claws\"](https://jwst-docs.stsci.edu/jwst-near-infrared-camera/nircam-instrument-features-and-caveats/nircam-claws-and-wisps).\n",
    "\n",
    "Before we start, let's first demonstrate how the data quality arrays work and explore some related pipeline convenience functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda62ab0-38a8-4bb5-a1c9-c05380196aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all available jwst pipeline data quality flags.\n",
    "# Data quality flags are added together in the image's data quality array, e.g. a DEAD and DO_NOT_USE pixel will\n",
    "# have a value of 1024+1 = 1025\n",
    "\n",
    "dqflags.pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31309798-d766-4056-98f9-cb4bc827a880",
   "metadata": {},
   "source": [
    "More details on the various data quality flags can be found [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html#data-quality-flags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f9685-df54-4855-9638-7bfb3f78ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to translate a specific value in an image's data quality array to words\n",
    "\n",
    "dqflags.dqflags_to_mnemonics(262657, dqflags.pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59919345-aaaa-4ebe-b731-fd0f3fb8cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on how to calculate the total number of pixels flagged as a certain bad pixel type and, alternatively,\n",
    "# how many pixels are NOT flagged as that bad pixel type.\n",
    "\n",
    "file = 'jw04443002001_02101_00004_nrcb3_rate.fits'\n",
    "dq = fits.getdata(file, 'DQ')\n",
    "\n",
    "n_dead = len(dq[dq&dqflags.pixel['DEAD']!=0])\n",
    "print('{} dead pixels in {}.'.format(n_dead, file))\n",
    "\n",
    "n_not_dead = len(dq[dq&dqflags.pixel['DEAD']==0])\n",
    "print('{} pixels NOT dead in {}.'.format(n_not_dead, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e4cf4d-bef5-4d1f-bb24-414a95527e58",
   "metadata": {},
   "source": [
    "In this example, we'll draw a region around the persistence area in ds9, and translate that region file into a mask with the same image dimensions as our data. We'll then flag the area as PERSISTENCE and DO_NOT_USE in each image's DQ arrays. The DO_NOT_USE flag is necessary to make the pipeline ignore the pixels during the image3 pipeline processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaef292-7fcb-4c2f-8782-a5c4d195ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to flag the region with high persistence.\n",
    "# If you don't want to bother with ds9 region files, you can simply manually create the polygon\n",
    "# list by inputting the x,y coordinates of the bad region.\n",
    "\n",
    "# How to create the persistence region file in ds9:\n",
    "# 1. Open the _cal image in ds9 (Analysis -> Smooth is useful here to smooth the image to ensure you flag the full extent \n",
    "# of the bad regions)\n",
    "# 2. Click Edit->Region and then click Region -> Shape -> Polygon\n",
    "# 3. Drawn polygon around persistence area\n",
    "# 4. Save the region file (Region -> Save Regions) as e.g. persistence.reg; when saving, set format=ds9, coord system=image\n",
    "\n",
    "with open('persistence.reg') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('polygon'):\n",
    "            coords = [float(coord) for coord in line.split('(')[1].split(')')[0].split(',')]\n",
    "            polygon = list(zip(coords[::2], coords[1::2]))\n",
    "            polygon.append(polygon[0])\n",
    "            print(polygon)\n",
    "\n",
    "# Create the persistence mask using the polygon region created above\n",
    "img = Image.new('L', (2048, 2048), 0)\n",
    "ImageDraw.Draw(img).polygon(polygon, outline=1, fill=1)\n",
    "pers_mask = np.array(img)\n",
    "plt.imshow(pers_mask, origin='lower', cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71758ff-c1e0-494b-9c32-0043861f4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag the persistence region in each image's DQ arrays as PERSISTENCE and DO_NOT_USE.\n",
    "\n",
    "files = sorted(glob.glob('*nrcb3_rate.fits'))\n",
    "for file in files:\n",
    "    h = fits.open(file)\n",
    "    original_dq = h['DQ'].data\n",
    "    new_dq = np.copy(original_dq)\n",
    "    # Add the PERSISTENCE and DO_NOT_USE flags to the pixels in the persistence region that are not already\n",
    "    # flagged as such. This method preserves any other existing dq flags.\n",
    "    new_dq[(pers_mask==1) & (original_dq&dqflags.pixel['PERSISTENCE']==0)] += dqflags.pixel['PERSISTENCE']\n",
    "    new_dq[(pers_mask==1) & (original_dq&dqflags.pixel['DO_NOT_USE']==0)] += dqflags.pixel['DO_NOT_USE']\n",
    "    h['DQ'].data = new_dq\n",
    "    h.writeto(file, overwrite=True)\n",
    "    h.close()\n",
    "\n",
    "# Plot the old and new data quality arrays for one of the images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax[0].imshow(original_dq, origin='lower', cmap='gray', vmin=4, vmax=5)\n",
    "ax[0].set_title('Original DQ')\n",
    "ax[1].imshow(new_dq, origin='lower', cmap='gray', vmin=4, vmax=5)\n",
    "ax[1].set_title('New DQ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fab8f-d11f-45a6-bf10-f0c56483c378",
   "metadata": {},
   "source": [
    "<a id='image2'></a>\n",
    "## Run the image2 pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17a7b4-daa8-4574-b1b9-a13ca91af5d9",
   "metadata": {},
   "source": [
    "Next we'll run the image2 pipeline using our modified rate images. The data quality modifications and custom snowball handling above will be carried over by default, so no custom settings are needed here.\n",
    "\n",
    "[Next Cell](#image2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ef153-4f56-4909-a348-cdd08c33652d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run image2 pipeline on our modified rate files\n",
    "\n",
    "rate_files = sorted(glob.glob('*nrcb3_rate.fits'))\n",
    "for file in rate_files:\n",
    "    result = Image2Pipeline.call(file, save_results=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5f04d-5070-4753-b621-f8da9d9ea2ac",
   "metadata": {},
   "source": [
    "<a id='image2_output'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04bb66-9525-4868-9177-293faacea9f2",
   "metadata": {},
   "source": [
    "<a id='1overf'></a>\n",
    "## Correct 1/f residuals and amplifier offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc90b7ea-69cd-4033-810f-19564cec3c5e",
   "metadata": {},
   "source": [
    "1/f noise is correlated noise that appears as horizontal banding in NIRCam images. While the [reference pixel step](https://jwst-pipeline.readthedocs.io/en/stable/jwst/refpix/description.html) in the detector1 pipeline is designed to correct this noise, it isn't perfect, and remaining horizontal banding can be seen throughout the later stages of the pipeline.\n",
    "\n",
    "Several [community tools](https://www.stsci.edu/jwst/science-planning/tools-from-the-community) have been created to correct this remaining 1/f noise in NIRCam images. In this notebook, we'll apply a basic correction by subtracting the source-masked, median-collapsed row values relative to the pedestal from each image. While we're correcting this 1/f noise, we'll also tweak the overall signal levels in the 4 amplifiers to better match eachother, as the borders between neighboring amplifiers often show an abrupt change in background levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5548bd2-ff77-4795-b5e3-058c9c9c6d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images show both 1/f noise residuals as horizontal bands and small amplifier offsets\n",
    "\n",
    "data = fits.getdata('jw04443002001_02101_00008_nrcb3_default_cal.fits')\n",
    "data_conv = convolve(data, Gaussian2DKernel(x_stddev=3))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(data_conv, cmap='gray', origin='lower', vmin=.25 ,vmax=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e808744-6aad-4b0d-8031-d7923966b87a",
   "metadata": {},
   "source": [
    "<a id='segmaps'></a>\n",
    "### Generate segmentation maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593b51e-4c50-418d-ba47-1038f4a80277",
   "metadata": {},
   "source": [
    "Before correcting the 1/f noise, we'll first need to generate segmentation maps for each input image. Because the sources appear more clearly in the longwave images, we'll generate the segmentation maps using the longwave data, and then use a pipeline function to blot back the results onto the shortwave images. The upper left quadrant of BLONG has ~the same field-of-view as B3 ([NIRCam FOV](https://jwst-docs.stsci.edu/jwst-near-infrared-camera/nircam-instrumentation/nircam-field-of-view))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e687133-583d-45ee-ba09-90c73287f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show SW image and corresponding LW image, to highlight why we're making segmentation maps using the LW\n",
    "\n",
    "data_sw = fits.getdata('jw04443002001_02101_00008_nrcb3_cal.fits')\n",
    "data_lw = fits.getdata('jw04443002001_02101_00008_nrcblong_cal.fits')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax[0].imshow(data_sw, origin='lower', cmap='gray', vmin=0.2, vmax=.6)\n",
    "ax[0].set_title('SW image', size=20)\n",
    "ax[1].imshow(data_lw[1024:,0:1024], origin='lower', cmap='gray', vmin=0.13, vmax=0.21)\n",
    "ax[1].set_title('LW image cutout', size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457551f8-8839-4467-a03f-b9fe27512a98",
   "metadata": {},
   "source": [
    "We'll generate segmentation maps for each longwave image by gaussian-smoothing the image and flagging all 8 interconnected pixels 1 sigma above the background as a source. To make sure we catch the wings of the large bright sources, we'll also gaussian-smooth the segmentation map itself before writing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c78cdf-8107-47e9-bf58-84b87d9cc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make segmentation maps for each corresponding NRCBLONG image\n",
    "\n",
    "files = sorted(glob.glob('*nrcb3_cal.fits'))\n",
    "for file in files:\n",
    "    # Find sources using corresponding lw image\n",
    "    lw_file = file.replace('nrcb3', 'nrcblong')\n",
    "    data  = fits.getdata(lw_file, 'SCI')\n",
    "    dq = fits.getdata(lw_file, 'DQ')\n",
    "    data = np.ma.masked_array(data, mask=dq!=0)  # avoids bad pixels as sources\n",
    "\n",
    "    # Make segmentation map\n",
    "    mean, median, stddev = sigma_clipped_stats(data, sigma=3.0)\n",
    "    data -= median  # subtract background\n",
    "    threshold = 1.0 * stddev\n",
    "    data_conv = convolve(data, Gaussian2DKernel(x_stddev=3))\n",
    "    segmap_orig = detect_sources(data_conv, threshold, npixels=8).data.astype(int)\n",
    "    segmap_orig[segmap_orig!=0] = 1\n",
    "\n",
    "    # Smooth segmap to catch faint wings of sources\n",
    "    segmap = convolve(segmap_orig, Gaussian2DKernel(x_stddev=3))\n",
    "    segmap[segmap<0.05] = 0\n",
    "    segmap[segmap>=0.05] = 1\n",
    "\n",
    "    # Write out the final segmap\n",
    "    fits.writeto(lw_file.replace('.fits', '_seg.fits'), segmap, overwrite=True)\n",
    "\n",
    "# Plot one of the images and its corresponding segmap\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "data[data.mask==True] = 0  # dont display maked pixels\n",
    "ax[0].imshow(data, origin='lower', cmap='gray', vmin=-.05, vmax=.05)\n",
    "ax[0].set_title('LW image', size=20)\n",
    "ax[1].imshow(segmap, origin='lower', cmap='gray', vmin=0, vmax=0.1)\n",
    "ax[1].set_title('LW image segmap', size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac19b3b-0a6d-42d5-ab0e-f8f20fd61b84",
   "metadata": {},
   "source": [
    "Next, we'll blot the segmentation maps from the longwave data onto the shortwave pixel-space, and write out the resulting segmentation map for each shortwave image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b7ff5-1276-40b7-a5d5-54ae12c147a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make segmentation maps for each SW image by blotting back the segmap from the corresponding LW image\n",
    "\n",
    "files = sorted(glob.glob('*nrcb3_cal.fits'))\n",
    "for file in files:\n",
    "    # Create an image model of the lw segmap\n",
    "    lw_file = file.replace('nrcb3', 'nrcblong')\n",
    "    lw_model = ImageModel(lw_file)\n",
    "    lw_segmap = fits.getdata(lw_file.replace('.fits', '_seg.fits'))\n",
    "    lw_model.data = lw_segmap\n",
    "    \n",
    "    # Blot the segmap data from the lw image onto the corresponding sw image\n",
    "    model = ImageModel(file)\n",
    "    blotted_data = gwcs_blot(lw_model, model, interp='nearest')\n",
    "    fits.writeto(file.replace('.fits', '_seg.fits'), blotted_data, overwrite=True)\n",
    "\n",
    "# Plot the sw image and segmap, as well as the corresponding lw segmap\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30,10))\n",
    "ax[0].imshow(model.data, origin='lower', cmap='gray', vmin=0.2, vmax=.6)\n",
    "ax[0].set_title('SW image', size=30)\n",
    "ax[1].imshow(blotted_data, origin='lower', cmap='gray', vmin=0, vmax=.1)\n",
    "ax[1].set_title('SW segmap', size=30)\n",
    "ax[2].imshow(lw_segmap, origin='lower', cmap='gray', vmin=0, vmax=.1)\n",
    "ax[2].set_title('LW segmap', size=30)\n",
    "ax[2].add_patch(Rectangle((0, 1024), 1024, 1024, linewidth=5, edgecolor='red', facecolor='none'))  # highlight rough SW FOV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f616e3a-db6f-4a47-b101-39844ed928ef",
   "metadata": {},
   "source": [
    "<a id='1overf_corr'></a>\n",
    "### Run the 1/f and amplifier offset correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee236c7-2d13-40dc-bab8-385279863a0b",
   "metadata": {},
   "source": [
    "We'll now correct the 1/f noise residuals and amplifier offsets in each shortwave image, using the segmentation maps created above for masking purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d32f5-7c72-4cdc-b952-db2aad442b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct 1/f noise residuals and amplifier offsets\n",
    "\n",
    "files = sorted(glob.glob('*nrcb3_cal.fits'))\n",
    "for file in files:\n",
    "    # Get data and segmap\n",
    "    h = fits.open(file)\n",
    "    data = h['SCI'].data\n",
    "    dq = h['DQ'].data\n",
    "    segmap = fits.getdata(file.replace('.fits', '_seg.fits'))\n",
    "\n",
    "    # Mask bad pixels, persistence, and sources\n",
    "    data_masked = np.copy(data)\n",
    "    data_masked[(dq!=0) | (segmap!=0)] = np.nan\n",
    "    clipped = sigma_clip(data_masked, sigma=3)\n",
    "    data_masked[clipped.mask==True] = np.nan\n",
    "\n",
    "    # Get full-frame median\n",
    "    med = np.nanmedian(data_masked)\n",
    "\n",
    "    # Get median-collapsed row/column offsets, representing the 1/f residuals and amp offsets.\n",
    "    # Bin the column offsets since they're larger-scale.\n",
    "    collapsed_rows = np.nanmedian(data_masked - med, axis=1)\n",
    "    collapsed_cols = np.nanmedian(data_masked - med, axis=0)\n",
    "    bin_size = 16\n",
    "    collapsed_cols_binned = [np.nanmedian(collapsed_cols[idx:idx+bin_size]) \n",
    "                             for idx in np.arange(0, len(collapsed_cols), bin_size)]\n",
    "\n",
    "    # Create a correction image combining the collapsed row/column offsets\n",
    "    correction_image = np.tile(np.repeat(collapsed_cols_binned, bin_size), (2048, 1)) + \\\n",
    "                       np.swapaxes(np.tile(collapsed_rows, (2048, 1)), 0, 1)\n",
    "    \n",
    "    # Apply the correction image to the original data\n",
    "    data_new = data - correction_image\n",
    "\n",
    "    # Write out the corrected file\n",
    "    h['SCI'].data = data_new\n",
    "    h.writeto(file.replace('.fits', '_corr.fits'), overwrite=True)\n",
    "    h.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4680690d-4952-4c37-8d20-ca81d3074bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one image, the correction applied to it, and the corrected image\n",
    "\n",
    "data = fits.getdata('jw04443002001_02101_00008_nrcb3_cal.fits')\n",
    "data_new = fits.getdata('jw04443002001_02101_00008_nrcb3_cal_corr.fits')\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30,10))\n",
    "ax[0].imshow(data, origin='lower', cmap='gray', vmin=0.2, vmax=.45)\n",
    "ax[0].set_title('Original', size=30)\n",
    "final_correction = data - data_new\n",
    "final_correction[~np.isfinite(final_correction)] = 0  # ignore nans in display\n",
    "ax[1].imshow(final_correction, origin='lower', cmap='gray', vmin=-.1, vmax=.1)\n",
    "ax[1].set_title('Model', size=30)\n",
    "ax[2].imshow(data_new, origin='lower', cmap='gray', vmin=0.2, vmax=.45)\n",
    "ax[2].set_title('Corrected', size=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d4fb46-607d-43f6-95a0-9eb65792853f",
   "metadata": {},
   "source": [
    "<a id='tweakreg'></a>\n",
    "## Correct tweakreg alignment issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519db2e0-0d22-4632-8b10-ec70b65f64bf",
   "metadata": {},
   "source": [
    "The drizzled image from the default pipeline run showed multiple images of individual sources, suggesting an issue with the image alignments in the image3 pipeline tweakreg step. Since the images were misaligned, the sources themselves in some images were flagged as outliers, and only their fainter wings can be seen in the final drizzled image. This can be confirmed by inspecting the cosmic ray flagged (i.e. _crf.fits) data quality arrays, which are generated for each input image during the image3 outlier detection step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5d1fa-1a39-42c8-9f1c-35b9bd64e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot default drizzled image cutout and the outlier flags in a single exposure\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "data = fits.getdata('nircam_f070w_default_i2d.fits', 'SCI')\n",
    "ax[0].imshow(data[500:1300, 250:1050], origin='lower', cmap='gray', vmin=0.2, vmax=.5)\n",
    "ax[0].set_title('Default Drizzle', size=15)\n",
    "data = fits.getdata('jw04443002001_02101_00008_nrcb3_default_a3001_crf.fits', 'SCI')\n",
    "ax[1].imshow(data[0:600, 0:600], origin='lower', cmap='gray', vmin=0.15, vmax=0.6)\n",
    "ax[1].set_title('Default Image', size=15)\n",
    "dq = fits.getdata('jw04443002001_02101_00008_nrcb3_default_a3001_crf.fits', 'DQ')\n",
    "dq[dq&dqflags.pixel['OUTLIER']==0] = 0  # ony show pixels flagged as OUTLIER\n",
    "ax[2].imshow(dq[0:600, 0:600], origin='lower', cmap='gray', vmin=0, vmax=0.1)\n",
    "ax[2].set_title('OUTLIER DQ flags', size=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e939d54-0b64-4ec0-ae83-7a359c2e7652",
   "metadata": {},
   "source": [
    "By inspecting the output logging from the default image3 pipeline run, we can see that most images failed to find enough sources for tweakreg alignment, so the tweakreg step was skipped for these images: \n",
    "\n",
    "<code>WARNING - Not enough matches (< 9) found for image catalog 'GROUP ID: jw04443002001_02101_00016_nrcb3_default_cal'\n",
    "</code>\n",
    "\n",
    "The couple images that did find enough sources had large unexpected shifts (XSH, YSH). This is not expected because aligning and blinking the images in ds9 (Frame -> Match -> Frame -> WCS) shows reasonable WCS alignment.\n",
    "\n",
    "<code>Computed 'shift' fit for GROUP ID: jw04443002001_02101_00008_nrcb3_default_cal:\n",
    "XSH: -0.575567  YSH: 2.02581</code>\n",
    "\n",
    "The issue here is likely due to a combination of this image having few good sources to match on, and tweakreg matching on bad pixels/sources instead. Let's plot the image and it's data quality array, and overlay the sources identified by tweakreg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e44490-2eff-4bc8-92bb-f9e8becd6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single image cutout and its jump flags, and overplot sources identified by tweakreg\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "data = fits.getdata('jw04443002001_02101_00008_nrcb3_default_cal.fits', 'SCI')\n",
    "dq = fits.getdata('jw04443002001_02101_00008_nrcb3_default_cal.fits', 'DQ')\n",
    "dq[dq&dqflags.pixel['JUMP_DET']==0] = 0  # only show pixels flagged as JUMP\n",
    "t = Table.read('jw04443002001_02101_00008_nrcb3_default_cal_cat.ecsv')\n",
    "ax[0].imshow(data, origin='lower', cmap='gray', vmin=0.2, vmax=.5)\n",
    "ax[0].set_title('Default Image', size=30)\n",
    "ax[0].scatter(t['x'], t['y'], marker='o', facecolor='none', edgecolor='limegreen', s=500, linewidths=3)\n",
    "xlim, ylim = [1000,1500], [1775, 2044]\n",
    "ax[0].set_xlim(xlim)\n",
    "ax[0].set_ylim(ylim)\n",
    "ax[1].imshow(dq, origin='lower', cmap='gray', vmin=0, vmax=.1)\n",
    "ax[1].set_title('JUMP DQ Flags', size=30)\n",
    "ax[1].set_xlim(xlim)\n",
    "ax[1].set_ylim(ylim)\n",
    "ax[1].scatter(t['x'], t['y'], marker='o', facecolor='none', edgecolor='limegreen', s=500, linewidths=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af15b17-4d58-4b02-a1eb-aabfcaa2c935",
   "metadata": {},
   "source": [
    "Here we see that every source identified by tweakreg is on top of a JUMP_DET flag. These sources only show up in a single image in WCS-space, so we know they are not real sources. Let's see what's happening up-the-ramp for one of these outlier JUMP_DET pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d7570-407f-4c99-9b6c-a2c0d22340d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ramp_data('jw04443002001_02101_00004_nrcb3_rate.fits', 1152, 1357, cutout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90420171-0578-4c5d-a944-bd8d0a9995c7",
   "metadata": {},
   "source": [
    "As seen in the ramp data, the jump step does not appear to be handling these cosmic rays correctly. Group 2 is correctly flagged as a jump, but group 3 is not, resulting in group 3 being included in the ramp fit step and thus a high value in the rate image. \n",
    "\n",
    "Group 4 is also mysteriously flagged as a jump even though it look like not much signal was accumulated between groups 3 and 4. This is potentially an issue with how the pipeline handles images with a low number of groups; the jump step works by comparing group differences to the median group difference, and in this case a low difference between groups (e.g. groups 3 to 4) is actually an outlier compared to the more common, larger group differences seen between the other groups. Group 2 may be flagged just due to being such a high jump compared to the median group differences.\n",
    "\n",
    "This case highlights the importance of, if possible, increasing the number of groups/int in programs. At the moment, there is no pipeline setting to better handle these scenarios, so we'll instead create custom catalogs for each image after cleaning these bad pixels, and feed these custom catalogs into the tweakreg step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e3331-96ff-4d96-9739-17efa4c9eaa1",
   "metadata": {},
   "source": [
    "<a id='catalogs'></a>\n",
    "### Generate custom source catalogs for tweakreg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23774018-5c42-4561-8c0a-2bee117018d2",
   "metadata": {},
   "source": [
    "Here we'll create custom source catalogs for each input image. To avoid identifying those bad jump pixels described above as sources, we'll first identify them as a pixel flagged as either JUMP_DET or DO_NOT_USE and whose ratio of the original image to its gaussian-smoothed version is different by greater than 100%. Once identified, we'll replace these pixels with their gaussian-smoothed values and proceed with source identification on these cleaned images. We'll also avoid identifying anything within the persistence regions flagged previously as a source as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee6b29-64ea-41c0-802a-49e3ced4a124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create custom catalogs to feed into tweakreg, avoiding bad pixels as sources\n",
    "\n",
    "files = sorted(glob.glob('*nrcb3_cal_corr.fits'))\n",
    "# make sure displayed image file for this cell matches example file used in previous cells\n",
    "files = [files[0]] + files[2:] + [files[1]]\n",
    "for file in files:\n",
    "    data = fits.getdata(file, 'SCI')\n",
    "    data_orig = np.copy(data)\n",
    "    dq = fits.getdata(file, 'DQ')\n",
    "    mask = (dq&dqflags.pixel['JUMP_DET']!=0) | (dq&dqflags.pixel['DO_NOT_USE']!=0)\n",
    "    data_conv = convolve(data, Gaussian2DKernel(x_stddev=2), mask=mask)  # bad pixels not included in gaussian-fit\n",
    "    ratio = abs(1 - (data/data_conv))\n",
    "    data[(mask==True) & (ratio>1)] = data_conv[(mask==True) & (ratio>1)]  # replace bad pixels\n",
    "    data[dq&dqflags.pixel['PERSISTENCE']!=0] = np.nan  # ignore persistence region\n",
    "    mean, median, stddev = sigma_clipped_stats(data, sigma=3.0)\n",
    "    daofind = DAOStarFinder(fwhm=0.935, threshold=10*stddev, brightest=200, min_separation=10)\n",
    "    sources = daofind(data - median)\n",
    "    t = Table()\n",
    "    t['id'] = sources['id']\n",
    "    t['x'] = sources['xcentroid']\n",
    "    t['y'] = sources['ycentroid']\n",
    "    t['flux'] = sources['flux']\n",
    "    t.write(file.replace('.fits', '.ecsv'), format='ascii.ecsv', overwrite=True)\n",
    "\n",
    "# Plot cutout of one of the original and cleaned images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "data_orig[~np.isfinite(data_orig)] = 0  # ignore nans\n",
    "ax[0].imshow(data_orig, origin='lower', cmap='gray', vmin=0.2, vmax=.5)\n",
    "ax[0].set_title('Original', size=30)\n",
    "xlim, ylim = [1000,1500], [1775, 2044]\n",
    "ax[0].set_xlim(xlim)\n",
    "ax[0].set_ylim(ylim)\n",
    "data[~np.isfinite(data)] = 0  # ignore nans\n",
    "ax[1].imshow(data, origin='lower', cmap='gray', vmin=0.2, vmax=.5)\n",
    "ax[1].set_title('Cleaned', size=30)\n",
    "ax[1].set_xlim(xlim)\n",
    "ax[1].set_ylim(ylim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392d205-80b9-4e00-867c-de20a48e69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the new catalogs are ignoring JUMPS now and finding real sources\n",
    "\n",
    "# Plot a single image cutout and its jump flags, and overplot sources identified by tweakreg\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "data = fits.getdata('jw04443002001_02101_00008_nrcb3_default_cal.fits', 'SCI')\n",
    "dq = fits.getdata('jw04443002001_02101_00008_nrcb3_default_cal.fits', 'DQ')\n",
    "dq[dq&dqflags.pixel['JUMP_DET']==0] = 0  # only show pixels flagged as JUMP\n",
    "t = Table.read('jw04443002001_02101_00008_nrcb3_cal_corr.ecsv')\n",
    "ax[0].imshow(data, origin='lower', cmap='gray', vmin=0.2, vmax=.5)\n",
    "ax[0].set_title('Default Image', size=30)\n",
    "ax[0].scatter(t['x'], t['y'], marker='o', facecolor='none', edgecolor='limegreen', s=500, linewidths=3)\n",
    "xlim, ylim = [1000,1500], [1775, 2044]\n",
    "ax[0].set_xlim(xlim)\n",
    "ax[0].set_ylim(ylim)\n",
    "ax[1].imshow(dq, origin='lower', cmap='gray', vmin=0, vmax=.1)\n",
    "ax[1].set_title('JUMP DQ Flags', size=30)\n",
    "ax[1].set_xlim(xlim)\n",
    "ax[1].set_ylim(ylim)\n",
    "ax[1].scatter(t['x'], t['y'], marker='o', facecolor='none', edgecolor='limegreen', s=500, linewidths=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd3808-4303-4445-98ff-2018519472de",
   "metadata": {},
   "source": [
    "As seen above, these new catalogs are now finding several good sources to align on and are generally avoiding the bad pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3aa99-2537-4712-b24d-0164097e762b",
   "metadata": {},
   "source": [
    "<a id='image3'></a>\n",
    "### Run the image3 pipeline with the custom tweakreg catalogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e908ed-9d54-4c07-94bc-d9a4291c4f72",
   "metadata": {},
   "source": [
    "Now we'll run the final stage of the pipeline, image3, to combine all of our custom images into a final drizzled product. We need to make sure to feed in our custom catalogs created above to the tweakreg step. To do this, we need to both add the catalog name to the ```tweakreg_catalog``` parameter in the association file for each member and set ```use_custom_catalogs``` to True in the tweakreg step.\n",
    "\n",
    "Since the images have reasonable alignment by default, we'll also decrease the search radius and tolerance values for source matching, and because we're still dealing with a small number of good sources in these images we'll decrease the minimum number of objects to match on. The tweakreg parameters are all described in detail [here](https://jwst-pipeline.readthedocs.io/en/latest/jwst/tweakreg/README.html#step-arguments).\n",
    "\n",
    "[Next Cell](#image3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef102d9-9c24-4c7f-8e3b-5047a4a68077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create association file for image3 using custom catalogs\n",
    "cal_files = sorted(glob.glob('*nrcb3_cal_corr.fits'))\n",
    "asn = asn_from_list.asn_from_list(cal_files, rule=DMS_Level3_Base, product_name='nircam_f070w')\n",
    "for member in asn['products'][0]['members']:  # add tweakreg catalogs to asn file\n",
    "    member['tweakreg_catalog'] = member['expname'].replace('.fits', '.ecsv')\n",
    "with open('nircam_f070w.json', 'w') as outfile:\n",
    "    outfile.write(asn.dump()[1])\n",
    "\n",
    "# Run image3 pipeline\n",
    "result = Image3Pipeline.call('nircam_f070w.json', save_results=True, steps={'tweakreg': {'use_custom_catalogs': True,\n",
    "                                                                                         'minobj': 8,\n",
    "                                                                                         'searchrad': 1,\n",
    "                                                                                         'tolerance': 0.5}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce925b-f7c8-42b3-ae76-c6880d0380b7",
   "metadata": {},
   "source": [
    "<a id='image3_output'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a70906-996d-4475-b7e7-df7819ce8a1c",
   "metadata": {},
   "source": [
    "As seen in the output log above, the tweakreg step is now successfully aligning all of the images, and the shifts applied all appear reasonable (XSH,YSH ~0.03\" or ~1 pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a4aa1-96a6-4f0b-8985-8c534609427a",
   "metadata": {},
   "source": [
    "<a id='compare'></a>\n",
    "## Compare the results of the default and custom pipeline runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7f063-4c84-4b75-ada9-4789ffd1567a",
   "metadata": {},
   "source": [
    "Below, we compare the final result of the default pipeline run to our custom pipeline run. In the custom image, the snowball/asteroid residuals are removed, the horizontal banding and amplifier offsets are decreased, the areas of high persistence are removed, and the images are well-aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c70be4-52ec-4302-a166-069a090b0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the default and custom drizzle\n",
    "\n",
    "data = fits.getdata('nircam_f070w_default_i2d.fits')\n",
    "data_new = fits.getdata('nircam_f070w_i2d.fits')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,10))\n",
    "ax[0].imshow(data, origin='lower', cmap='gray', vmin=0.2, vmax=.45)\n",
    "ax[0].set_title('Default', size=30)\n",
    "ax[1].imshow(data_new, origin='lower', cmap='gray', vmin=0.2, vmax=.45)\n",
    "ax[1].set_title('Custom', size=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8d28e-b097-42c0-9866-d4a90ee5dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot median-collapsed row values of default and custom drizzles\n",
    "\n",
    "collapsed = np.nanmedian(data[700:1300, :], axis=0)\n",
    "x = np.arange(len(collapsed))\n",
    "plt.scatter(x, collapsed, label='Default', alpha=0.1)\n",
    "collapsed = np.nanmedian(data_new[700:1300, :], axis=0)\n",
    "x = np.arange(len(collapsed))\n",
    "plt.scatter(x, collapsed, label='Custom', alpha=0.1)\n",
    "plt.ylim(.28, .34)\n",
    "plt.legend()\n",
    "plt.xlabel('Column #')\n",
    "plt.ylabel('Signal [MJy/sr]')\n",
    "plt.grid(ls='--', color='gray', alpha=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a7998-505a-48d8-921e-49b3f1dbe4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms and print image stats of default and custom drizzles\n",
    "\n",
    "h = plt.hist(data.flatten(), bins=100, range=(.2,.45), alpha=0.5, label='Default')\n",
    "h = plt.hist(data_new.flatten(), bins=100, range=(.2,.45), alpha=0.5, label='Custom')\n",
    "plt.legend()\n",
    "plt.ylabel('Number of Pixels')\n",
    "plt.xlabel('Signal [MJy/sr]')\n",
    "plt.grid(ls='--', color='gray', alpha=0.5)\n",
    "\n",
    "# print out image stats\n",
    "mean, med, stddev = sigma_clipped_stats(data[1250:1750, 1250:1750], sigma=3)\n",
    "print('Default \\n mean: {:.4f} \\n med: {:.4f} \\n stddev: {:.5f}'.format(mean, med, stddev))\n",
    "mean, med, stddev = sigma_clipped_stats(data_new[1250:1750, 1250:1750], sigma=3)\n",
    "print('Custom \\n mean: {:.4f} \\n med: {:.4f} \\n stddev: {:.5f}'.format(mean, med, stddev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a449ef-845b-47e3-8e52-d3ddb10bb0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "masterclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
